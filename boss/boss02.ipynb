{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab1a2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: å¯¼å…¥åº“\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80401e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 2: çˆ¬è™«ç±»\n",
    "class BossSpiderAPI:\n",
    "    def __init__(self):\n",
    "        \"\"\"é‡‘èè¡Œä¸šèŒä½çˆ¬è™« - æ”¯æŒæ–­ç‚¹ç»­çˆ¬ (ä¼˜åŒ–ç‰ˆ)\"\"\"\n",
    "        self.base_url = \"https://www.zhipin.com/wapi/zpgeek/search/joblist.json\"\n",
    "        \n",
    "        # ï¼ï¼ï¼æ¯æ¬¡è§¦å‘é£æ§åï¼Œæ›´æ–°è¿™é‡Œçš„ Cookieï¼ï¼ï¼\n",
    "        # ä¼˜åŒ–ï¼šä¿®æ­£ User-Agent ä¸ºä¸»æµç¨³å®šç‰ˆæœ¬ï¼Œæ·»åŠ  Sec å¤´æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "            \"Referer\": \"https://www.zhipin.com/web/geek/job\",\n",
    "            \"Accept\": \"application/json, text/plain, */*\",\n",
    "            # \"Content-Type\": \"application/x-www-form-urlencoded\", # requestsä¼šè‡ªåŠ¨å¤„ç†ï¼Œæ— éœ€æ‰‹åŠ¨è®¾ç½®\n",
    "            \"Sec-Ch-Ua\": '\"Google Chrome\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"',\n",
    "            \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "            \"Sec-Ch-Ua-Platform\": '\"Windows\"',\n",
    "            \"Sec-Fetch-Dest\": \"empty\",\n",
    "            \"Sec-Fetch-Mode\": \"cors\",\n",
    "            \"Sec-Fetch-Site\": \"same-origin\",\n",
    "            \"Cookie\": \"ab_guid=78d319c7-13e6-4a47-baf0-f120a7373d6f; lastCity=100010000; wt2=Dkk1F98VQmqMn0UgGwTzC7o7OoHUbYqVCcZHsIQC0DajHRfhzMgYzgV38ljcbzyds0dOdNNbVyWhaO38G_gxErg~~; wbg=0; zp_at=LZazoHzQtE86zsLEFo3PmN_T2jRcXSEvG-dxQ9Lh-JI~; bst=V2R9MnF-L5219pVtRuyRgYLSy27DrTxS0~|R9MnF-L5219pVtRuyRgYLSy27DrRzSk~; __zp_stoken__=9a9agPErDpMODwprDgz8pO0U8NkE4QTxKQz5HPEpZLz0%2FQThLRhkkw4PChmUnYsOIw7TDhz03QzhFPEpBOD5BJ0NEPT9KODk9xL7DgUA5wrBQxIU9KCvDlcOBwoVrJWTDkQwrCcKTw4EMaSzEgcOKDMKnwr0oMRIPWQwIXWQWDA9nDw8PZBYUXhIPZl9fYmpcCBsMZAgTHRw8w63Cv8Kow4fDtMK8wq7DhcOyw4XCrcOLPT9BODM5xL7CsDBLSj1EQ0s4xL7FgMS5xYLFg8S%2BxYDDicOaw7zDqsWAxLnFgsK8xL7FgMS5xILDnMS%2BxYDEucWCw73CvzNDwprCvcO5w4rDsnfDrMSdwqDCocKYT8OyXsKxcMOGwr%2FCsknDgsOBwp7CtMKWw4rCuk%2FCrMKvUsKuwpldwr5mSFt6WFFUa2lnTmFaSXZZGxFeeV4QQwkrSMOP\" \n",
    "            # æ³¨æ„ï¼šè¯·åŠ¡å¿…å¡«å…¥æœ€æ–°çš„ Cookieï¼Œå¦åˆ™å¿…æŠ¥ 37 é”™è¯¯\n",
    "        }\n",
    "        \n",
    "        self.finance_pattern = re.compile(\n",
    "            r\"(é‡‘è|é“¶è¡Œ|åˆ¸å•†|è¯åˆ¸|åŸºé‡‘|ä¿é™©|æœŸè´§|ä¿¡æ‰˜|æŠ•è¡Œ|èµ„ç®¡|ç†è´¢|è´¢å¯Œ|\"\n",
    "            r\"å°è´·|æ¶ˆé‡‘|ä¿ç†|ç§Ÿèµ|å¾ä¿¡|æ¸…ç®—|æ”¯ä»˜|äº’é‡‘|é‡‘èç§‘æŠ€|FinTech)\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        self.progress_file = \"job_data_IN_PROGRESS.xlsx\"\n",
    "        self.state_file = \"crawl_state.txt\"\n",
    "        self.detail_state_file = \"detail_state.txt\"\n",
    "        \n",
    "        self.data_list = []\n",
    "        self.seen_ids = set()\n",
    "\n",
    "    def is_finance_related(self, job):\n",
    "        \"\"\"åˆ¤æ–­æ˜¯å¦ä¸ºé‡‘èç›¸å…³èŒä½\"\"\"\n",
    "        text = \" \".join([\n",
    "            str(job.get(\"brandIndustry\", \"\") or \"\"),\n",
    "            str(job.get(\"brandName\", \"\") or \"\"),\n",
    "            str(job.get(\"jobName\", \"\") or \"\")\n",
    "        ])\n",
    "        return bool(self.finance_pattern.search(text))\n",
    "\n",
    "    def load_crawl_state(self, keywords):\n",
    "        \"\"\"åŠ è½½çˆ¬å–çŠ¶æ€\"\"\"\n",
    "        if not os.path.exists(self.state_file):\n",
    "            return 0, 1\n",
    "        \n",
    "        try:\n",
    "            with open(self.state_file, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) < 2: return 0, 1\n",
    "                keyword = lines[0].strip()\n",
    "                page = int(lines[1].strip())\n",
    "                \n",
    "                if keyword in keywords:\n",
    "                    idx = keywords.index(keyword)\n",
    "                    print(f\"ğŸ“ æ¢å¤åˆ—è¡¨çˆ¬å–è¿›åº¦ï¼šå…³é”®è¯ [{keyword}] ç¬¬ {page} é¡µ\")\n",
    "                    return idx, page\n",
    "                else:\n",
    "                    return 0, 1\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}ï¼Œä»å¤´å¼€å§‹\")\n",
    "            return 0, 1\n",
    "\n",
    "    def save_crawl_state(self, keyword, page):\n",
    "        \"\"\"ä¿å­˜çˆ¬å–çŠ¶æ€\"\"\"\n",
    "        try:\n",
    "            with open(self.state_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"{keyword}\\n{page}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def load_detail_state(self):\n",
    "        \"\"\"åŠ è½½è¯¦æƒ…æŠ“å–è¿›åº¦\"\"\"\n",
    "        if not os.path.exists(self.detail_state_file):\n",
    "            return 0\n",
    "        try:\n",
    "            with open(self.detail_state_file, 'r', encoding='utf-8') as f:\n",
    "                index = int(f.read().strip())\n",
    "                print(f\"ğŸ“ æ¢å¤è¯¦æƒ…æŠ“å–è¿›åº¦ï¼šä»ç¬¬ {index + 1} æ¡å¼€å§‹\")\n",
    "                return index\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    def save_detail_state(self, index):\n",
    "        \"\"\"ä¿å­˜è¯¦æƒ…æŠ“å–è¿›åº¦\"\"\"\n",
    "        try:\n",
    "            with open(self.detail_state_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(str(index))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def fetch_data(self, query=\"é‡‘è\", start_page=1, max_pages=5):\n",
    "        \"\"\"æŠ“å–èŒä½åˆ—è¡¨æ•°æ®\"\"\"\n",
    "        for page in range(start_page, max_pages + 1):\n",
    "            print(f\"\\nğŸ” æŠ“å–å…³é”®è¯ [{query}] åˆ—è¡¨ç¬¬ {page} é¡µ...\")\n",
    "            \n",
    "            # éšæœºé•¿æš‚åœï¼Œæ¨¡æ‹Ÿäººç±»ç¿»é¡µæ€è€ƒ\n",
    "            if page > 1 and page % 5 == 0:\n",
    "                 time.sleep(random.uniform(5.0, 8.0))\n",
    "\n",
    "            params = {\n",
    "                \"scene\": \"1\",\n",
    "                \"query\": query,\n",
    "                \"city\": \"100010000\",\n",
    "                \"page\": page,\n",
    "                \"pageSize\": 30\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                resp = requests.post(\n",
    "                    self.base_url, \n",
    "                    headers=self.headers, \n",
    "                    data=params, # requests ä¼šè‡ªåŠ¨æ·»åŠ  form-urlencoded å¤´\n",
    "                    timeout=15\n",
    "                )\n",
    "                resp.raise_for_status()\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"âŒ è¯·æ±‚å¤±è´¥ï¼š{e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = resp.json()\n",
    "                code = result.get(\"code\")\n",
    "                msg = result.get(\"message\") or result.get(\"msg\")\n",
    "                \n",
    "                if code == 37:\n",
    "                    print(f\"âš ï¸ è§¦å‘é£æ§ (code: 37)ï¼\")\n",
    "                    print(f\"ğŸ“ å½“å‰è¿›åº¦ï¼šå…³é”®è¯ [{query}] ç¬¬ {page} é¡µ\")\n",
    "                    self.save_crawl_state(query, page)\n",
    "                    self.save_progress()\n",
    "                    return False\n",
    "                elif code not in (0, \"0\", None):\n",
    "                    print(f\"âš ï¸ æ¥å£å¼‚å¸¸ code: {code}, message: {msg}\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ è§£æå“åº”å¤±è´¥: {e}\")\n",
    "                continue\n",
    "\n",
    "            job_list = result.get(\"zpData\", {}).get(\"jobList\", [])\n",
    "            \n",
    "            if not job_list:\n",
    "                print(\"âš ï¸ æœ¬é¡µæ— æ•°æ®ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªå…³é”®è¯\")\n",
    "                break\n",
    "            \n",
    "            print(f\"âœ… è·å–åˆ° {len(job_list)} ä¸ªèŒä½\")\n",
    "            \n",
    "            finance_count = 0\n",
    "            for job in job_list:\n",
    "                job_id = job.get(\"encryptJobId\")\n",
    "                \n",
    "                if not job_id or job_id in self.seen_ids:\n",
    "                    continue\n",
    "                \n",
    "                if not self.is_finance_related(job):\n",
    "                    continue\n",
    "                \n",
    "                finance_count += 1\n",
    "                \n",
    "                item = {\n",
    "                    \"èŒä½\": job.get(\"jobName\"),\n",
    "                    \"å…¬å¸\": job.get(\"brandName\"),\n",
    "                    \"è–ªèµ„\": job.get(\"salaryDesc\"),\n",
    "                    \"åœ°åŒº\": job.get(\"cityName\"),\n",
    "                    \"ç»éªŒ\": job.get(\"jobExperience\"),\n",
    "                    \"å­¦å†\": job.get(\"jobDegree\"),\n",
    "                    \"å…¬å¸è§„æ¨¡\": job.get(\"brandScaleName\"),\n",
    "                    \"è¡Œä¸š\": job.get(\"brandIndustry\"),\n",
    "                    \"ç¦åˆ©æ ‡ç­¾\": \",\".join(job.get(\"welfareList\", []) or []),\n",
    "                    \"æŠ€èƒ½æ ‡ç­¾\": \",\".join(job.get(\"skills\", []) or []),\n",
    "                    \"èŒä½æè¿°\": \"\",\n",
    "                    \"job_id\": job_id\n",
    "                }\n",
    "                \n",
    "                self.data_list.append(item)\n",
    "                self.seen_ids.add(job_id)\n",
    "            \n",
    "            print(f\"ğŸ“Š æœ¬é¡µç¬¦åˆæ¡ä»¶: {finance_count} ä¸ª | ç´¯è®¡: {len(self.data_list)} ä¸ª\")\n",
    "            \n",
    "            self.save_crawl_state(query, page + 1)\n",
    "            self.save_progress()\n",
    "            \n",
    "            # ç¨å¾®å»¶é•¿ç­‰å¾…æ—¶é—´ä»¥é™ä½é£æ§æ¦‚ç‡\n",
    "            time.sleep(random.uniform(3.0, 6.0))\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def get_job_detail(self, job_id):\n",
    "        \"\"\"è·å–èŒä½è¯¦æƒ…æè¿° - å¢å¼ºç‰ˆ\"\"\"\n",
    "        url = f\"https://www.zhipin.com/job_detail/{job_id}.html\"\n",
    "        \n",
    "        # å¤åˆ¶ä¸» Header å¹¶é’ˆå¯¹è¯¦æƒ…é¡µåšå¾®è°ƒ\n",
    "        detail_headers = self.headers.copy()\n",
    "        detail_headers.update({\n",
    "            \"Referer\": \"https://www.zhipin.com/web/geek/job\", # æ¨¡æ‹Ÿæ¥æº\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\"\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            resp = requests.get(url, headers=detail_headers, timeout=15)\n",
    "            \n",
    "            if resp.status_code == 404:\n",
    "                return \"èŒä½å·²å…³é—­æˆ–ä¸å­˜åœ¨\"\n",
    "            if resp.status_code == 403:\n",
    "                print(f\"  âš ï¸ 403 ç¦æ­¢è®¿é—®\")\n",
    "                return None\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"  âš ï¸ çŠ¶æ€ç : {resp.status_code}\")\n",
    "                return None\n",
    "            \n",
    "            if 'security-check' in resp.url or 'verify' in resp.url:\n",
    "                print(f\"  âš ï¸ è§¦å‘å®‰å…¨éªŒè¯è·³è½¬\")\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            \n",
    "            selectors = [\n",
    "                '.job-sec-text', '.job-detail-section .text', '.job-sec .text',\n",
    "                '.job-description', '.job-detail .text', '[class*=\"job-sec\"] .text'\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                desc_tag = soup.select_one(selector)\n",
    "                if desc_tag and desc_tag.text.strip():\n",
    "                    return desc_tag.text.strip()\n",
    "            \n",
    "            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥æ˜¯å¦é¡µé¢åŠ è½½äº†ä½†ç»“æ„å˜äº†\n",
    "            if \"èŒä½æè¿°\" in resp.text:\n",
    "                print(f\"  âš ï¸ é¡µé¢å«å…³é”®è¯ä½†æœªåŒ¹é…åˆ°é€‰æ‹©å™¨\")\n",
    "            return \"\"\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  âš ï¸ è¯¦æƒ…é¡µè¯·æ±‚å¼‚å¸¸: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_progress(self):\n",
    "        \"\"\"ä»è¿›åº¦æ–‡ä»¶ä¸­åŠ è½½æ•°æ®\"\"\"\n",
    "        print(f\"\\nğŸ”„ æ­£åœ¨ä» {self.progress_file} åŠ è½½è¿›åº¦...\")\n",
    "        try:\n",
    "            df = pd.read_excel(self.progress_file)\n",
    "            df['èŒä½æè¿°'] = df['èŒä½æè¿°'].fillna('')\n",
    "            df['job_id'] = df['job_id'].fillna('')\n",
    "            \n",
    "            self.data_list = df.to_dict('records')\n",
    "            self.seen_ids = {item['job_id'] for item in self.data_list if item.get('job_id')}\n",
    "            \n",
    "            print(f\"âœ… åŠ è½½æˆåŠŸï¼Œå·²æ¢å¤ {len(self.data_list)} æ¡è®°å½•ã€‚\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"â„¹ï¸ æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "\n",
    "    def save_progress(self):\n",
    "        \"\"\"å°†å½“å‰æ•°æ®ä¿å­˜åˆ°è¿›åº¦æ–‡ä»¶\"\"\"\n",
    "        if not self.data_list:\n",
    "            return\n",
    "        try:\n",
    "            df = pd.DataFrame(self.data_list)\n",
    "            df.to_excel(self.progress_file, index=False, na_rep='')\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {e}\")\n",
    "\n",
    "    def fetch_all_details_resumable(self, start_index=0):\n",
    "        \"\"\"å¯æ¢å¤åœ°æŠ“å–æ‰€æœ‰è¯¦æƒ… - é€»è¾‘ä¼˜åŒ–ç‰ˆ\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸš€ (é˜¶æ®µ2) å¼€å§‹æŠ“å–èŒä½è¯¦ç»†æè¿°...\")\n",
    "        if start_index > 0:\n",
    "            print(f\"ğŸ“ è·³è¿‡å‰ {start_index} æ¡ï¼Œç›´æ¥ä»ç¬¬ {start_index + 1} æ¡å¼€å§‹\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        fetched_count = 0\n",
    "        failed_count = 0\n",
    "        consecutive_fail = 0\n",
    "        \n",
    "        # ä¼˜åŒ–ï¼šç›´æ¥ä» start_index å¼€å§‹åˆ‡ç‰‡éå†ï¼Œé¿å…æ— æ•ˆå¾ªç¯\n",
    "        total_items = len(self.data_list)\n",
    "        \n",
    "        for i in range(start_index, total_items):\n",
    "            item = self.data_list[i]\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æè¿°ï¼ˆæ•°æ®æ¢å¤çš„æƒ…å†µï¼‰\n",
    "            desc_str = str(item.get(\"èŒä½æè¿°\", \"\")).strip()\n",
    "            if desc_str and desc_str.lower() != 'nan' and len(desc_str) > 10:\n",
    "                continue\n",
    "            \n",
    "            print(f\"  ({i+1}/{total_items}) [æŠ“å–] {item['èŒä½']} - {item['å…¬å¸']}\")\n",
    "            \n",
    "            job_id = item.get(\"job_id\")\n",
    "            if not job_id:\n",
    "                continue\n",
    "            \n",
    "            new_desc = self.get_job_detail(job_id)\n",
    "            \n",
    "            if new_desc is None: # å¤±è´¥\n",
    "                consecutive_fail += 1\n",
    "                failed_count += 1\n",
    "                print(f\"  âŒ æŠ“å–å¤±è´¥ (è¿ç»­: {consecutive_fail})\")\n",
    "                self.save_detail_state(i) # è®°å½•å½“å‰å¤±è´¥ä½ç½®ï¼Œä¸‹æ¬¡ä»è¿™é‡Œå¼€å§‹\n",
    "                \n",
    "                if consecutive_fail >= 5:\n",
    "                    print(f\"\\n  ğŸ›‘ è¿ç»­å¤±è´¥è§¦å‘é£æ§ä¿æŠ¤ï¼Œåœæ­¢è¿è¡Œã€‚\")\n",
    "                    self.save_progress()\n",
    "                    return i\n",
    "                \n",
    "                time.sleep(random.uniform(10.0, 20.0)) # å¤±è´¥åé•¿ç­‰å¾…\n",
    "                \n",
    "            elif new_desc == \"\": # ç©ºå†…å®¹\n",
    "                consecutive_fail = 0\n",
    "                item[\"èŒä½æè¿°\"] = \"[æœªè·å–åˆ°æè¿°]\"\n",
    "                print(f\"  âš ï¸ æœªæ‰¾åˆ°æè¿°å†…å®¹\")\n",
    "                time.sleep(random.uniform(2.0, 4.0))\n",
    "                \n",
    "            else: # æˆåŠŸ\n",
    "                consecutive_fail = 0\n",
    "                item[\"èŒä½æè¿°\"] = new_desc\n",
    "                fetched_count += 1\n",
    "                print(f\"  âœ… è·å–æˆåŠŸ\")\n",
    "                \n",
    "                # æˆåŠŸåä¿å­˜çŠ¶æ€ï¼ˆæ¯5æ¡ä¿å­˜ä¸€æ¬¡ï¼Œå‡å°‘IOï¼‰\n",
    "                if fetched_count % 5 == 0:\n",
    "                    self.save_progress()\n",
    "                    self.save_detail_state(i + 1)\n",
    "                \n",
    "                time.sleep(random.uniform(3.0, 6.0)) # æ­£å¸¸é—´éš”\n",
    "        \n",
    "        # å¾ªç¯ç»“æŸåçš„ä¿å­˜\n",
    "        self.save_progress()\n",
    "        self.save_detail_state(total_items)\n",
    "        \n",
    "        print(\"\\nâœ… (é˜¶æ®µ2) è¯¦æƒ…æŠ“å–å®Œæ¯•ã€‚\")\n",
    "        if os.path.exists(self.detail_state_file):\n",
    "            os.remove(self.detail_state_file)\n",
    "        \n",
    "        return -1\n",
    "\n",
    "    def save_final_excel(self):\n",
    "        \"\"\"ä¿å­˜ä¸ºæœ€ç»ˆçš„ Excel æ–‡ä»¶\"\"\"\n",
    "        if not self.data_list:\n",
    "            return\n",
    "        \n",
    "        final_data = [item.copy() for item in self.data_list]\n",
    "        for item in final_data:\n",
    "            item.pop('job_id', None) # ç§»é™¤ job_id\n",
    "                \n",
    "        df = pd.DataFrame(final_data)\n",
    "        filename = f\"é‡‘èè¡Œä¸šå²—ä½_å®Œæ•´ç‰ˆ_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.xlsx\"\n",
    "        df.to_excel(filename, index=False)\n",
    "        print(f\"\\nğŸ‰ å…¨éƒ¨å®Œæˆï¼å·²ä¿å­˜ï¼š{filename} (å…± {len(df)} æ¡)\")\n",
    "\n",
    "    def run(self, keywords, max_pages_per_kw=10, skip_list=False):\n",
    "        \"\"\"ä¸»è¿è¡Œé€»è¾‘\"\"\"\n",
    "        # åŠ è½½æ•°æ®\n",
    "        if os.path.exists(self.progress_file):\n",
    "            self.load_progress()\n",
    "        \n",
    "        # é˜¶æ®µ1ï¼šæŠ“å–åˆ—è¡¨\n",
    "        if not skip_list:\n",
    "            start_kw_idx, start_page = self.load_crawl_state(keywords)\n",
    "            \n",
    "            print(f\"\\nğŸš€ (é˜¶æ®µ1) èŒä½åˆ—è¡¨æŠ“å–\")\n",
    "            for kw_idx in range(start_kw_idx, len(keywords)):\n",
    "                keyword = keywords[kw_idx]\n",
    "                page = start_page if kw_idx == start_kw_idx else 1\n",
    "                \n",
    "                print(f\"\\n>>> æ­£åœ¨æŠ“å–: {keyword} ({kw_idx+1}/{len(keywords)})\")\n",
    "                success = self.fetch_data(query=keyword, start_page=page, max_pages=max_pages_per_kw)\n",
    "                \n",
    "                if not success:\n",
    "                    print(\"\\nâš ï¸ ç¨‹åºæš‚åœï¼šè¯·æ›´æ–° Cookie åé‡æ–°è¿è¡Œ\")\n",
    "                    return\n",
    "                \n",
    "                time.sleep(random.uniform(2.0, 4.0))\n",
    "            \n",
    "            if os.path.exists(self.state_file):\n",
    "                os.remove(self.state_file)\n",
    "        \n",
    "        # é˜¶æ®µ2ï¼šæŠ“å–è¯¦æƒ…\n",
    "        start_detail_index = self.load_detail_state()\n",
    "        if self.data_list:\n",
    "            result = self.fetch_all_details_resumable(start_index=start_detail_index)\n",
    "            if result >= 0:\n",
    "                print(\"\\nâš ï¸ è¯¦æƒ…æŠ“å–æš‚åœï¼šè¯·æ›´æ–° Cookie åé‡æ–°è¿è¡Œ\")\n",
    "                return\n",
    "        \n",
    "        self.save_final_excel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a10626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ æ­£åœ¨ä» job_data_IN_PROGRESS.xlsx åŠ è½½è¿›åº¦...\n",
      "âœ… åŠ è½½æˆåŠŸï¼Œå·²æ¢å¤ 1749 æ¡è®°å½•ã€‚\n",
      "ğŸ“ æ¢å¤åˆ—è¡¨çˆ¬å–è¿›åº¦ï¼šå…³é”®è¯ [æ•°æ®åˆ†æ] ç¬¬ 4 é¡µ\n",
      "\n",
      "ğŸš€ (é˜¶æ®µ1) èŒä½åˆ—è¡¨æŠ“å–\n",
      "\n",
      ">>> æ­£åœ¨æŠ“å–: æ•°æ®åˆ†æ (1/9)\n",
      "\n",
      "ğŸ” æŠ“å–å…³é”®è¯ [æ•°æ®åˆ†æ] åˆ—è¡¨ç¬¬ 4 é¡µ...\n",
      "âœ… è·å–åˆ° 30 ä¸ªèŒä½\n",
      "ğŸ“Š æœ¬é¡µç¬¦åˆæ¡ä»¶: 0 ä¸ª | ç´¯è®¡: 1749 ä¸ª\n",
      "\n",
      "ğŸ” æŠ“å–å…³é”®è¯ [æ•°æ®åˆ†æ] åˆ—è¡¨ç¬¬ 5 é¡µ...\n",
      "âœ… è·å–åˆ° 30 ä¸ªèŒä½\n",
      "ğŸ“Š æœ¬é¡µç¬¦åˆæ¡ä»¶: 0 ä¸ª | ç´¯è®¡: 1749 ä¸ª\n",
      "\n",
      "ğŸ” æŠ“å–å…³é”®è¯ [æ•°æ®åˆ†æ] åˆ—è¡¨ç¬¬ 6 é¡µ...\n",
      "âœ… è·å–åˆ° 30 ä¸ªèŒä½\n",
      "ğŸ“Š æœ¬é¡µç¬¦åˆæ¡ä»¶: 0 ä¸ª | ç´¯è®¡: 1749 ä¸ª\n",
      "\n",
      "ğŸ” æŠ“å–å…³é”®è¯ [æ•°æ®åˆ†æ] åˆ—è¡¨ç¬¬ 7 é¡µ...\n",
      "âœ… è·å–åˆ° 30 ä¸ªèŒä½\n",
      "ğŸ“Š æœ¬é¡µç¬¦åˆæ¡ä»¶: 0 ä¸ª | ç´¯è®¡: 1749 ä¸ª\n",
      "\n",
      "ğŸ” æŠ“å–å…³é”®è¯ [æ•°æ®åˆ†æ] åˆ—è¡¨ç¬¬ 8 é¡µ...\n",
      "âš ï¸ è§¦å‘é£æ§ (code: 37)ï¼\n",
      "ğŸ“ å½“å‰è¿›åº¦ï¼šå…³é”®è¯ [æ•°æ®åˆ†æ] ç¬¬ 8 é¡µ\n",
      "\n",
      "âš ï¸ ç¨‹åºæš‚åœï¼šè¯·æ›´æ–° Cookie åé‡æ–°è¿è¡Œ\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: è¿è¡Œçˆ¬è™«\n",
    "if __name__ == \"__main__\":\n",
    "    spider = BossSpiderAPI()\n",
    "    \n",
    "    # ä¼˜åŒ–ï¼šå»é™¤äº†é‡å¤çš„å…³é”®è¯\n",
    "    raw_keywords = [\"æ•°æ®åˆ†æ\", \"åŸºé‡‘ç»ç†\", \"äº§å“\", \"è¿è¥\", \"è¯åˆ¸ç ”ç©¶æ‰€\", \"æŠ•è¡Œ\", \"é‡‘è\", \"é“¶è¡Œ\", \"ä¿é™©\"]\n",
    "    # ä¿æŒé¡ºåºå»é‡\n",
    "    unique_keywords = list(dict.fromkeys(raw_keywords))\n",
    "    \n",
    "    # æ­£å¸¸è¿è¡Œ\n",
    "    spider.run(keywords=unique_keywords, max_pages_per_kw=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
