{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "312038f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02709c1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Retry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 491\u001b[39m\n\u001b[32m    487\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ğŸ—‘ï¸  å·²åˆ é™¤çŠ¶æ€æ–‡ä»¶ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     spider = \u001b[43mBossSpiderAPI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     keywords = [\u001b[33m\"\u001b[39m\u001b[33mæ•°æ®åˆ†æ\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33måŸºé‡‘ç»ç†\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mäº§å“\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mè¿è¥\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mè¯åˆ¸ç ”ç©¶æ‰€\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mæŠ•è¡Œ\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mé‡‘è\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mé“¶è¡Œ\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mä¿é™©\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    495\u001b[39m     spider.run(keywords=keywords, max_pages_per_kw=\u001b[32m10\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mBossSpiderAPI.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mself\u001b[39m.proxies = []\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# åˆ›å»º Session å¹¶é…ç½®é‡è¯•\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28mself\u001b[39m.session = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mBossSpiderAPI._create_session\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ Session\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m session = requests.Session()\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m retry = \u001b[43mRetry\u001b[49m(\n\u001b[32m     53\u001b[39m     total=\u001b[32m3\u001b[39m,\n\u001b[32m     54\u001b[39m     backoff_factor=\u001b[32m1\u001b[39m,\n\u001b[32m     55\u001b[39m     status_forcelist=[\u001b[32m500\u001b[39m, \u001b[32m502\u001b[39m, \u001b[32m503\u001b[39m, \u001b[32m504\u001b[39m],\n\u001b[32m     56\u001b[39m     allowed_methods=[\u001b[33m\"\u001b[39m\u001b[33mGET\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     57\u001b[39m )\n\u001b[32m     58\u001b[39m adapter = HTTPAdapter(max_retries=retry)\n\u001b[32m     59\u001b[39m session.mount(\u001b[33m'\u001b[39m\u001b[33mhttps://\u001b[39m\u001b[33m'\u001b[39m, adapter)\n",
      "\u001b[31mNameError\u001b[39m: name 'Retry' is not defined"
     ]
    }
   ],
   "source": [
    "class BossSpiderAPI:\n",
    "    def __init__(self):\n",
    "        \"\"\"é‡‘èè¡Œä¸šèŒä½çˆ¬è™« - æ”¯æŒæ–­ç‚¹ç»­çˆ¬\"\"\"\n",
    "        self.base_url = \"https://www.zhipin.com/wapi/zpgeek/search/joblist.json\"\n",
    "        \n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36\",\n",
    "            \"Referer\": \"https://www.zhipin.com/web/geek/job\",\n",
    "            \"Accept\": \"application/json, text/plain, */*\",\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "            \"Cookie\": \"ab_guid=78d319c7-13e6-4a47-baf0-f120a7373d6f; lastCity=100010000; wt2=Dkk1F98VQmqMn0UgGwTzC7o7OoHUbYqVCcZHsIQC0DajHRfhzMgYzgV38ljcbzyds0dOdNNbVyWhaO38G_gxErg~~; wbg=0; zp_at=LZazoHzQtE86zsLEFo3PmN_T2jRcXSEvG-dxQ9Lh-JI~; bst=V2R9MnF-L5219pVtRuyRgYLSy27DrRzSk~|R9MnF-L5219pVtRuyRgYLSy27Drexyw~; __zp_stoken__=0a91gPEDDpMK5wpbCvUMvE0s8NEc8RzxAPT5BPEBfLUM5RThBQh9Xwr3Cig0kWMOIScK5Qzs9PEs8QEc8OkcrPUBDOUBDQ0PEusOFQEPCsFDEgUMsJcKEw4DCjw8iZMOVDMO6w4MNw6PCuA0pDWvCvQ1Gwrw7KBMPXRAXXmYUEBJgDxMVYxcWXBISZ19jYl1dChcQYwkTGSBLw67Cv8Kuwr3Ds8K9wqjDgcOywrzCrsOLOUU4OTM9xYbCrzFLQEdDPEs%2BxYbEv8S6xLzEv8WGxL%2FDisOUw7rDmsS%2FxLrEvMK6xYbEv8S6w7zDmsWGxL%2FEusS8w7nDhTY8wpvCucKiw6fCsV7DqsSnwp%2FCrsKTwqnCt1DCpcK6wp7CuMO8dMKzdsK3XMKtwqpMWsKZUGLCgMKYwofCusK4wp5KUFZhXcK0TE1XYU3DgVNxFRMRelgTQRBkPsOT\"\n",
    "}\n",
    "        \n",
    "        self.detail_headers = {\n",
    "            \"User-Agent\": self.headers[\"User-Agent\"],\n",
    "            \"Cookie\": self.headers[\"Cookie\"]\n",
    "        }\n",
    "        \n",
    "        self.finance_pattern = re.compile(\n",
    "            r\"(é‡‘è|é“¶è¡Œ|åˆ¸å•†|è¯åˆ¸|åŸºé‡‘|ä¿é™©|æœŸè´§|ä¿¡æ‰˜|æŠ•è¡Œ|èµ„ç®¡|ç†è´¢|è´¢å¯Œ|\"\n",
    "            r\"å°è´·|æ¶ˆé‡‘|ä¿ç†|ç§Ÿèµ|å¾ä¿¡|æ¸…ç®—|æ”¯ä»˜|äº’é‡‘|é‡‘èç§‘æŠ€|FinTech)\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        self.progress_file = \"job_data_IN_PROGRESS.xlsx\"\n",
    "        self.state_file = \"crawl_state.txt\"  # æ–°å¢ï¼šè®°å½•çˆ¬å–çŠ¶æ€\n",
    "        \n",
    "        self.data_list = []\n",
    "        self.seen_ids = set()\n",
    "        \n",
    "        # æ–°å¢ï¼šè®°å½•çˆ¬å–è¿›åº¦\n",
    "        self.current_keyword_index = 0\n",
    "        self.current_page = 1\n",
    "\n",
    "    def is_finance_related(self, job):\n",
    "        \"\"\"åˆ¤æ–­æ˜¯å¦ä¸ºé‡‘èç›¸å…³èŒä½\"\"\"\n",
    "        text = \" \".join([\n",
    "            job.get(\"brandIndustry\", \"\") or \"\",\n",
    "            job.get(\"brandName\", \"\") or \"\",\n",
    "            job.get(\"jobName\", \"\") or \"\"\n",
    "        ])\n",
    "        return bool(self.finance_pattern.search(text))\n",
    "\n",
    "    def load_crawl_state(self, keywords):\n",
    "        \"\"\"åŠ è½½çˆ¬å–çŠ¶æ€ï¼ˆå½“å‰å…³é”®è¯å’Œé¡µç ï¼‰\"\"\"\n",
    "        if not os.path.exists(self.state_file):\n",
    "            return 0, 1  # é»˜è®¤ä»ç¬¬ä¸€ä¸ªå…³é”®è¯ç¬¬ä¸€é¡µå¼€å§‹\n",
    "        \n",
    "        try:\n",
    "            with open(self.state_file, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                keyword = lines[0].strip()\n",
    "                page = int(lines[1].strip())\n",
    "                \n",
    "                # æ‰¾åˆ°å…³é”®è¯åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•\n",
    "                if keyword in keywords:\n",
    "                    idx = keywords.index(keyword)\n",
    "                    print(f\"ğŸ“ æ¢å¤çˆ¬å–è¿›åº¦ï¼šå…³é”®è¯ [{keyword}] ç¬¬ {page} é¡µ\")\n",
    "                    return idx, page\n",
    "                else:\n",
    "                    print(f\"âš ï¸ çŠ¶æ€æ–‡ä»¶ä¸­çš„å…³é”®è¯ [{keyword}] ä¸åœ¨å½“å‰åˆ—è¡¨ä¸­ï¼Œä»å¤´å¼€å§‹\")\n",
    "                    return 0, 1\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}ï¼Œä»å¤´å¼€å§‹\")\n",
    "            return 0, 1\n",
    "\n",
    "    def save_crawl_state(self, keyword, page):\n",
    "        \"\"\"ä¿å­˜çˆ¬å–çŠ¶æ€\"\"\"\n",
    "        try:\n",
    "            with open(self.state_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"{keyword}\\n{page}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ä¿å­˜çŠ¶æ€å¤±è´¥: {e}\")\n",
    "\n",
    "    def fetch_data(self, query=\"é‡‘è\", start_page=1, max_pages=5):\n",
    "        \"\"\"\n",
    "        æŠ“å–èŒä½åˆ—è¡¨æ•°æ®\n",
    "        \n",
    "        å‚æ•°:\n",
    "            query: æœç´¢å…³é”®è¯\n",
    "            start_page: èµ·å§‹é¡µç ï¼ˆç”¨äºæ–­ç‚¹ç»­çˆ¬ï¼‰\n",
    "            max_pages: æœ€å¤§æŠ“å–é¡µæ•°\n",
    "        \"\"\"\n",
    "        for page in range(start_page, max_pages + 1):\n",
    "            print(f\"\\nğŸ” æŠ“å–å…³é”®è¯ [{query}] åˆ—è¡¨ç¬¬ {page} é¡µ...\")\n",
    "            \n",
    "            params = {\n",
    "                \"scene\": \"1\",\n",
    "                \"query\": query,\n",
    "                \"city\": \"100010000\",\n",
    "                \"page\": page,\n",
    "                \"pageSize\": 30\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                resp = requests.post(\n",
    "                    self.base_url, \n",
    "                    headers=self.headers, \n",
    "                    data=params,\n",
    "                    timeout=15\n",
    "                )\n",
    "                resp.raise_for_status()\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"âŒ è¯·æ±‚å¤±è´¥ï¼š{e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = resp.json()\n",
    "                code = result.get(\"code\")\n",
    "                msg = result.get(\"message\") or result.get(\"msg\")\n",
    "                \n",
    "                if code == 37:\n",
    "                    print(f\"âš ï¸ è§¦å‘é£æ§ (code: 37)ï¼\")\n",
    "                    print(f\"ğŸ“ å½“å‰è¿›åº¦ï¼šå…³é”®è¯ [{query}] ç¬¬ {page} é¡µ\")\n",
    "                    self.save_crawl_state(query, page)  # ä¿å­˜å½“å‰çŠ¶æ€\n",
    "                    self.save_progress()\n",
    "                    return False  # è¿”å› False è¡¨ç¤ºè§¦å‘é£æ§\n",
    "                elif code not in (0, \"0\", None):\n",
    "                    print(f\"âš ï¸ æ¥å£å¼‚å¸¸ code: {code}, message: {msg}\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ è§£æå“åº”å¤±è´¥: {e}\")\n",
    "                print(f\"å“åº”å†…å®¹: {resp.text[:300]}\")\n",
    "                continue\n",
    "\n",
    "            job_list = result.get(\"zpData\", {}).get(\"jobList\", [])\n",
    "            \n",
    "            if not job_list:\n",
    "                print(\"âš ï¸ æœ¬é¡µæ— æ•°æ®ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªå…³é”®è¯\")\n",
    "                break\n",
    "            \n",
    "            print(f\"âœ… è·å–åˆ° {len(job_list)} ä¸ªèŒä½\")\n",
    "            \n",
    "            finance_count = 0\n",
    "            for job in job_list:\n",
    "                job_id = job.get(\"encryptJobId\")\n",
    "                \n",
    "                if not job_id or job_id in self.seen_ids:\n",
    "                    continue\n",
    "                \n",
    "                if not self.is_finance_related(job):\n",
    "                    continue\n",
    "                \n",
    "                finance_count += 1\n",
    "                \n",
    "                item = {\n",
    "                    \"èŒä½\": job.get(\"jobName\"),\n",
    "                    \"å…¬å¸\": job.get(\"brandName\"),\n",
    "                    \"è–ªèµ„\": job.get(\"salaryDesc\"),\n",
    "                    \"åœ°åŒº\": job.get(\"cityName\"),\n",
    "                    \"ç»éªŒ\": job.get(\"jobExperience\"),\n",
    "                    \"å­¦å†\": job.get(\"jobDegree\"),\n",
    "                    \"å…¬å¸è§„æ¨¡\": job.get(\"brandScaleName\"),\n",
    "                    \"è¡Œä¸š\": job.get(\"brandIndustry\"),\n",
    "                    \"ç¦åˆ©æ ‡ç­¾\": \",\".join(job.get(\"welfareList\", []) or []),\n",
    "                    \"æŠ€èƒ½æ ‡ç­¾\": \",\".join(job.get(\"skills\", []) or []),\n",
    "                    \"èŒä½æè¿°\": \"\",\n",
    "                    \"job_id\": job_id\n",
    "                }\n",
    "                \n",
    "                self.data_list.append(item)\n",
    "                self.seen_ids.add(job_id)\n",
    "            \n",
    "            print(f\"ğŸ“Š æœ¬é¡µç¬¦åˆæ¡ä»¶: {finance_count} ä¸ª | ç´¯è®¡: {len(self.data_list)} ä¸ª\")\n",
    "            \n",
    "            # ä¿å­˜è¿›åº¦å’ŒçŠ¶æ€\n",
    "            self.save_crawl_state(query, page + 1)\n",
    "            self.save_progress()\n",
    "            \n",
    "            time.sleep(random.uniform(2.0, 3.0))\n",
    "        \n",
    "        return True  # è¿”å› True è¡¨ç¤ºæ­£å¸¸å®Œæˆ\n",
    "\n",
    "    def get_job_detail(self, job_id):\n",
    "        \"\"\"è·å–èŒä½è¯¦æƒ…æè¿°\"\"\"\n",
    "        url = f\"https://www.zhipin.com/job_detail/{job_id}.html\"\n",
    "        try:\n",
    "            resp = requests.get(url, headers=self.detail_headers, timeout=10)\n",
    "            if resp.status_code == 404:\n",
    "                return \"èŒä½å·²å…³é—­æˆ–ä¸å­˜åœ¨\"\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"  âš ï¸ è¯¦æƒ…é¡µçŠ¶æ€ç : {resp.status_code}\")\n",
    "                return None\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            desc_tag = soup.select_one('.job-sec-text')\n",
    "            return desc_tag.text.strip() if desc_tag else \"\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  âš ï¸ è¯¦æƒ…é¡µè¯·æ±‚å¤±è´¥: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_progress(self):\n",
    "        \"\"\"ä»è¿›åº¦æ–‡ä»¶ä¸­åŠ è½½æ•°æ®\"\"\"\n",
    "        print(f\"\\nğŸ”„ æ­£åœ¨ä» {self.progress_file} åŠ è½½è¿›åº¦...\")\n",
    "        try:\n",
    "            df = pd.read_excel(self.progress_file)\n",
    "            \n",
    "            df['èŒä½æè¿°'] = df['èŒä½æè¿°'].fillna('')\n",
    "            df['job_id'] = df['job_id'].fillna('')\n",
    "            \n",
    "            self.data_list = df.to_dict('records')\n",
    "            \n",
    "            for item in self.data_list:\n",
    "                if item.get('job_id'):\n",
    "                    self.seen_ids.add(item['job_id'])\n",
    "            \n",
    "            completed = sum(1 for item in self.data_list \n",
    "                           if item.get('èŒä½æè¿°') and \n",
    "                           str(item['èŒä½æè¿°']).strip() and \n",
    "                           str(item['èŒä½æè¿°']).strip().lower() != 'nan' and\n",
    "                           len(str(item['èŒä½æè¿°']).strip()) > 10)\n",
    "            \n",
    "            print(f\"âœ… åŠ è½½æˆåŠŸï¼Œå·²æ¢å¤ {len(self.data_list)} æ¡è®°å½•ã€‚\")\n",
    "            print(f\"   å…¶ä¸­ {completed} æ¡å·²æœ‰æè¿°ï¼Œ{len(self.data_list) - completed} æ¡å¾…æŠ“å–ã€‚\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"â„¹ï¸ æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "\n",
    "    def save_progress(self):\n",
    "        \"\"\"å°†å½“å‰æ•°æ®ä¿å­˜åˆ°è¿›åº¦æ–‡ä»¶\"\"\"\n",
    "        if not self.data_list:\n",
    "            return\n",
    "        try:\n",
    "            df = pd.DataFrame(self.data_list)\n",
    "            df.to_excel(self.progress_file, index=False, na_rep='')\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def fetch_all_details_resumable(self, start_index=0):\n",
    "        \"\"\"\n",
    "        å¯æ¢å¤åœ°æŠ“å–æ‰€æœ‰è¯¦æƒ…\n",
    "        \n",
    "        å‚æ•°:\n",
    "            start_index: ä»ç¬¬å‡ æ¡å¼€å§‹æŠ“å–ï¼ˆ0-indexedï¼‰ï¼Œé»˜è®¤ä»ç¬¬0æ¡å¼€å§‹\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸš€ (é˜¶æ®µ2) å¼€å§‹æŠ“å–èŒä½è¯¦ç»†æè¿°...\")\n",
    "        if start_index > 0:\n",
    "            print(f\"ğŸ“ ä»ç¬¬ {start_index + 1} æ¡å¼€å§‹ï¼ˆè·³è¿‡å‰ {start_index} æ¡ï¼‰\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        fetched_count = 0\n",
    "        skipped_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        for i, item in enumerate(self.data_list):\n",
    "            # *** æ–°å¢ï¼šè·³è¿‡æŒ‡å®šç´¢å¼•ä¹‹å‰çš„è®°å½• ***\n",
    "            if i < start_index:\n",
    "                skipped_count += 1\n",
    "                if skipped_count % 100 == 0:\n",
    "                    print(f\"  [å¿«é€Ÿè·³è¿‡] å·²è·³è¿‡ {skipped_count} æ¡...\")\n",
    "                continue\n",
    "            \n",
    "            desc = item.get(\"èŒä½æè¿°\", \"\")\n",
    "            desc_str = str(desc).strip() if desc else \"\"\n",
    "            \n",
    "            # å·²æœ‰æœ‰æ•ˆæè¿°çš„ä¹Ÿè·³è¿‡\n",
    "            if desc_str and desc_str.lower() != 'nan' and len(desc_str) > 10:\n",
    "                skipped_count += 1\n",
    "                if skipped_count % 10 == 1:\n",
    "                    print(f\"  ({i+1}/{len(self.data_list)}) [è·³è¿‡] å·²æœ‰æè¿°\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  ({i+1}/{len(self.data_list)}) [æŠ“å–] {item['èŒä½']} - {item['å…¬å¸']}\")\n",
    "            \n",
    "            job_id = item.get(\"job_id\")\n",
    "            if not job_id:\n",
    "                print(\"  âš ï¸ job_id ä¸ºç©ºï¼Œè·³è¿‡\")\n",
    "                continue\n",
    "            \n",
    "            new_desc = self.get_job_detail(job_id)\n",
    "            \n",
    "            if new_desc is None:\n",
    "                failed_count += 1\n",
    "                print(\"  âŒ è¯¦æƒ…é¡µæŠ“å–å¤±è´¥ï¼Œå¯èƒ½å·²è§¦å‘é£æ§ï¼\")\n",
    "                \n",
    "                if failed_count >= 3:\n",
    "                    print(f\"\\n  ğŸ›‘ è¿ç»­å¤±è´¥ {failed_count} æ¬¡ï¼Œåœæ­¢æŠ“å–ã€‚\")\n",
    "                    print(f\"  ğŸ’¡ å½“å‰ä½ç½®ï¼šç¬¬ {i+1} æ¡\")\n",
    "                    print(f\"  ğŸ’¡ ä¸‹æ¬¡è¿è¡Œæ—¶è®¾ç½® start_index={i}\")\n",
    "                    self.save_progress()\n",
    "                    return i  # è¿”å›å½“å‰ç´¢å¼•ï¼Œæ–¹ä¾¿ä¸‹æ¬¡ç»­çˆ¬\n",
    "                else:\n",
    "                    print(f\"  âš ï¸ å°†ç»§ç»­å°è¯•ä¸‹ä¸€æ¡ï¼ˆå¤±è´¥è®¡æ•°ï¼š{failed_count}/3ï¼‰\")\n",
    "                    time.sleep(random.uniform(5.0, 8.0))\n",
    "                    continue\n",
    "            \n",
    "            failed_count = 0\n",
    "            item[\"èŒä½æè¿°\"] = new_desc\n",
    "            fetched_count += 1\n",
    "            \n",
    "            self.save_progress()\n",
    "            time.sleep(random.uniform(4.0, 7.0))\n",
    "        \n",
    "        print(\"\\nâœ… (é˜¶æ®µ2) è¯¦æƒ…æŠ“å–å®Œæ¯•ã€‚\")\n",
    "        print(f\"   è·³è¿‡: {skipped_count} æ¡ | æ–°å¢: {fetched_count} æ¡ | å¤±è´¥: {failed_count} æ¡\")\n",
    "        return -1  # è¿”å› -1 è¡¨ç¤ºå…¨éƒ¨å®Œæˆ\n",
    "    \n",
    "    \n",
    "    def save_final_excel(self):\n",
    "        \"\"\"ä¿å­˜ä¸ºæœ€ç»ˆçš„ Excel æ–‡ä»¶\"\"\"\n",
    "        if not self.data_list:\n",
    "            print(\"\\nâŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜\")\n",
    "            return\n",
    "        \n",
    "        final_data = [item.copy() for item in self.data_list]\n",
    "        for item in final_data:\n",
    "            if 'job_id' in item:\n",
    "                del item['job_id']\n",
    "                \n",
    "        df = pd.DataFrame(final_data)\n",
    "        \n",
    "        filename = f\"é‡‘èè¡Œä¸šå²—ä½_å®Œæ•´ç‰ˆ_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "        df.to_excel(filename, index=False)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ‰ å…¨éƒ¨å®Œæˆï¼å·²ä¿å­˜æœ€ç»ˆæ–‡ä»¶ï¼š{filename}\")\n",
    "        print(f\"    ğŸ“¦ å…± {len(df)} æ¡èŒä½\")\n",
    "        print(f\"    ğŸ“‹ åŒ…å«åˆ—: {', '.join(df.columns.tolist())}\")\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, keywords=[\"é‡‘è\"], max_pages_per_kw=10, start_detail_index=0):\n",
    "        \"\"\"\n",
    "        è¿è¡Œçˆ¬è™«ï¼ˆæ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼‰\n",
    "        \n",
    "        å‚æ•°:\n",
    "            keywords: å…³é”®è¯åˆ—è¡¨\n",
    "            max_pages_per_kw: æ¯ä¸ªå…³é”®è¯æœ€å¤§é¡µæ•°\n",
    "            start_detail_index: è¯¦æƒ…æŠ“å–ä»ç¬¬å‡ æ¡å¼€å§‹ï¼ˆ0-indexedï¼‰\n",
    "        \"\"\"\n",
    "        \n",
    "        # æ­¥éª¤ 1: åŠ è½½å·²æœ‰æ•°æ®\n",
    "        if os.path.exists(self.progress_file):\n",
    "            self.load_progress()\n",
    "        \n",
    "        # æ­¥éª¤ 2: åŠ è½½çˆ¬å–çŠ¶æ€\n",
    "        start_kw_idx, start_page = self.load_crawl_state(keywords)\n",
    "        \n",
    "        # æ­¥éª¤ 3: ç»§ç»­æŠ“å–åˆ—è¡¨ï¼ˆä»æ–­ç‚¹å¤„å¼€å§‹ï¼‰\n",
    "        print(f\"\\nğŸš€ (é˜¶æ®µ1) å¼€å§‹/ç»§ç»­æŠ“å–èŒä½åˆ—è¡¨...\")\n",
    "        \n",
    "        for kw_idx in range(start_kw_idx, len(keywords)):\n",
    "            keyword = keywords[kw_idx]\n",
    "            page = start_page if kw_idx == start_kw_idx else 1\n",
    "            \n",
    "            print(f\"\\n{'='*40}\")\n",
    "            print(f\"å…³é”®è¯ ({kw_idx+1}/{len(keywords)}): {keyword}\")\n",
    "            print(f\"{'='*40}\")\n",
    "            \n",
    "            success = self.fetch_data(query=keyword, start_page=page, max_pages=max_pages_per_kw)\n",
    "            \n",
    "            if not success:\n",
    "                print(\"\\nâš ï¸ è§¦å‘é£æ§ï¼Œå·²ä¿å­˜å½“å‰è¿›åº¦ã€‚\")\n",
    "                print(\"ğŸ’¡ è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š\")\n",
    "                print(\"   1. æ›´æ–°ä»£ç ä¸­çš„ Cookie\")\n",
    "                print(\"   2. ç­‰å¾… 30-60 åˆ†é’Ÿ\")\n",
    "                print(\"   3. é‡æ–°è¿è¡Œæ­¤ç¨‹åºï¼ˆå°†è‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­ï¼‰\")\n",
    "                return\n",
    "            \n",
    "            if kw_idx < len(keywords) - 1:\n",
    "                time.sleep(random.uniform(3.0, 5.0))\n",
    "        \n",
    "        print(\"\\nğŸ‘ (é˜¶æ®µ1) èŒä½åˆ—è¡¨æŠ“å–å®Œæ¯•ã€‚\")\n",
    "        \n",
    "        # æ­¥éª¤ 4: æŠ“å–è¯¦æƒ…ï¼ˆæ”¯æŒæŒ‡å®šèµ·å§‹ä½ç½®ï¼‰\n",
    "        if self.data_list:\n",
    "            self.fetch_all_details_resumable(start_index=start_detail_index)\n",
    "        \n",
    "        # æ­¥éª¤ 5: ä¿å­˜æœ€ç»ˆæ–‡ä»¶å¹¶æ¸…ç†\n",
    "        self.save_final_excel()\n",
    "        \n",
    "        if os.path.exists(self.state_file):\n",
    "            os.remove(self.state_file)\n",
    "            print(f\"   ğŸ—‘ï¸  å·²åˆ é™¤çŠ¶æ€æ–‡ä»¶ {self.state_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spider = BossSpiderAPI()\n",
    "    \n",
    "    keywords = [\"é“¶è¡Œ\",\"ä¿é™©\"]\n",
    "    \n",
    "    # æ¯ä¸ªå…³é”®è¯æŠ“å– 10 é¡µ\n",
    "    spider.run(keywords=keywords, max_pages_per_kw=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
