{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1a2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: å¯¼å…¥åº“\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80401e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: çˆ¬è™«ç±»\n",
    "class BossSpiderAPI:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.zhipin.com/wapi/zpgeek/search/joblist.json\"\n",
    "        \n",
    "        # ä»æ–‡ä»¶è¯»å– Cookie\n",
    "        cookie = self._load_cookie_from_file()\n",
    "        \n",
    "        # å›ºå®š User-Agentï¼Œå‡å°‘å˜åŒ–å¸¦æ¥çš„é£é™©\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "            \"Referer\": \"https://www.zhipin.com/web/geek/job\",\n",
    "            \"Accept\": \"application/json, text/plain, */*\",\n",
    "            \"Cookie\": cookie\n",
    "        }\n",
    "        \n",
    "        self.finance_pattern = re.compile(\n",
    "            r\"(é‡‘è|é“¶è¡Œ|åˆ¸å•†|è¯åˆ¸|åŸºé‡‘|ä¿é™©|æœŸè´§|ä¿¡æ‰˜|æŠ•è¡Œ|èµ„ç®¡|ç†è´¢|è´¢å¯Œ|å°è´·|æ¶ˆé‡‘|ä¿ç†|ç§Ÿèµ|å¾ä¿¡|æ¸…ç®—|æ”¯ä»˜|äº’é‡‘|é‡‘èç§‘æŠ€|FinTech)\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        self.progress_file = \"job_data_IN_PROGRESS.xlsx\"\n",
    "        self.state_file = \"crawl_state.txt\"\n",
    "        self.detail_state_file = \"detail_state.txt\"\n",
    "        self.data_list = []\n",
    "        self.seen_ids = set()\n",
    "\n",
    "    def _load_cookie_from_file(self, filepath=\"cookie.txt\"):\n",
    "        \"\"\"ä»æ–‡ä»¶è¯»å– Cookie\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                cookie = f.read().strip()\n",
    "                # è¿‡æ»¤æ‰æ³¨é‡Šè¡Œ\n",
    "                lines = [line.strip() for line in cookie.split('\\n') \n",
    "                         if line.strip() and not line.strip().startswith('//')]\n",
    "                cookie = ''.join(lines)\n",
    "                if cookie:\n",
    "                    print(f\"âœ… æˆåŠŸä» {filepath} åŠ è½½ Cookie\")\n",
    "                    return cookie\n",
    "                else:\n",
    "                    print(f\"âš ï¸ {filepath} æ–‡ä»¶ä¸ºç©º\")\n",
    "                    return \"\"\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸ æœªæ‰¾åˆ° {filepath}ï¼Œè¯·åˆ›å»ºè¯¥æ–‡ä»¶å¹¶å¡«å…¥ Cookie\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è¯»å– Cookie æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _simple_sleep(self, min_sec=2.0, max_sec=4.0):\n",
    "        \"\"\"ç®€å•çš„éšæœºç­‰å¾…\"\"\"\n",
    "        time.sleep(random.uniform(min_sec, max_sec))\n",
    "\n",
    "    def is_finance_related(self, job):\n",
    "        \"\"\"åˆ¤æ–­èŒä½æ˜¯å¦ä¸é‡‘èç›¸å…³\"\"\"\n",
    "        check_fields = [\n",
    "            job.get(\"jobName\", \"\"),\n",
    "            job.get(\"brandName\", \"\"),\n",
    "            job.get(\"brandIndustry\", \"\"),\n",
    "        ]\n",
    "        combined_text = \" \".join(str(f) for f in check_fields if f)\n",
    "        return bool(self.finance_pattern.search(combined_text))\n",
    "\n",
    "    def load_crawl_state(self, keywords):\n",
    "        \"\"\"åŠ è½½çˆ¬å–çŠ¶æ€\"\"\"\n",
    "        if not os.path.exists(self.state_file):\n",
    "            return 0, 1\n",
    "        \n",
    "        try:\n",
    "            with open(self.state_file, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) < 2: return 0, 1\n",
    "                keyword = lines[0].strip()\n",
    "                page = int(lines[1].strip())\n",
    "                \n",
    "                if keyword in keywords:\n",
    "                    idx = keywords.index(keyword)\n",
    "                    print(f\"ğŸ“ æ¢å¤åˆ—è¡¨çˆ¬å–è¿›åº¦ï¼šå…³é”®è¯ [{keyword}] ç¬¬ {page} é¡µ\")\n",
    "                    return idx, page\n",
    "                else:\n",
    "                    return 0, 1\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}ï¼Œä»å¤´å¼€å§‹\")\n",
    "            return 0, 1\n",
    "\n",
    "    def save_crawl_state(self, keyword, page):\n",
    "        \"\"\"ä¿å­˜çˆ¬å–çŠ¶æ€\"\"\"\n",
    "        try:\n",
    "            with open(self.state_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"{keyword}\\n{page}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def load_detail_state(self):\n",
    "        \"\"\"åŠ è½½è¯¦æƒ…æŠ“å–è¿›åº¦\"\"\"\n",
    "        if not os.path.exists(self.detail_state_file):\n",
    "            return 0\n",
    "        try:\n",
    "            with open(self.detail_state_file, 'r', encoding='utf-8') as f:\n",
    "                index = int(f.read().strip())\n",
    "                print(f\"ğŸ“ æ¢å¤è¯¦æƒ…æŠ“å–è¿›åº¦ï¼šä»ç¬¬ {index + 1} æ¡å¼€å§‹\")\n",
    "                return index\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    def save_detail_state(self, index):\n",
    "        \"\"\"ä¿å­˜è¯¦æƒ…æŠ“å–è¿›åº¦\"\"\"\n",
    "        try:\n",
    "            with open(self.detail_state_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(str(index))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def fetch_data(self, query=\"é‡‘è\", start_page=1, max_pages=5):\n",
    "        \"\"\"æŠ“å–èŒä½åˆ—è¡¨ - ç®€åŒ–ç‰ˆ\"\"\"\n",
    "        consecutive_fail = 0\n",
    "        \n",
    "        for page in range(start_page, max_pages + 1):\n",
    "            print(f\"\\nğŸ” [{query}] ç¬¬ {page} é¡µ...\")\n",
    "            \n",
    "            params = {\n",
    "                \"scene\": \"1\", \"query\": query, \"city\": \"100010000\",\n",
    "                \"page\": page, \"pageSize\": 30\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                resp = requests.post(self.base_url, headers=self.headers, data=params, timeout=15)\n",
    "                result = resp.json()\n",
    "                code = result.get(\"code\")\n",
    "                \n",
    "                if code == 37:\n",
    "                    print(f\"âš ï¸ é£æ§è§¦å‘ï¼ä¿å­˜è¿›åº¦åé€€å‡º\")\n",
    "                    self.save_crawl_state(query, page)\n",
    "                    self.save_progress()\n",
    "                    return False\n",
    "                    \n",
    "                if code not in (0, \"0\", None):\n",
    "                    print(f\"âš ï¸ å¼‚å¸¸ code={code}\")\n",
    "                    consecutive_fail += 1\n",
    "                    if consecutive_fail >= 5:\n",
    "                        break\n",
    "                    self._simple_sleep(5, 10)\n",
    "                    continue\n",
    "                    \n",
    "                consecutive_fail = 0  # é‡ç½®å¤±è´¥è®¡æ•°\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ è¯·æ±‚å¤±è´¥: {e}\")\n",
    "                consecutive_fail += 1\n",
    "                if consecutive_fail >= 5:\n",
    "                    break\n",
    "                self._simple_sleep(3, 6)\n",
    "                continue\n",
    "\n",
    "            job_list = result.get(\"zpData\", {}).get(\"jobList\", [])\n",
    "            if not job_list:\n",
    "                print(\"âš ï¸ æ— æ•°æ®ï¼Œè·³è¿‡\")\n",
    "                break\n",
    "            \n",
    "            finance_count = 0\n",
    "            for job in job_list:\n",
    "                job_id = job.get(\"encryptJobId\")\n",
    "                if not job_id or job_id in self.seen_ids:\n",
    "                    continue\n",
    "                if not self.is_finance_related(job):\n",
    "                    continue\n",
    "                \n",
    "                finance_count += 1\n",
    "                self.data_list.append({\n",
    "                    \"èŒä½\": job.get(\"jobName\"),\n",
    "                    \"å…¬å¸\": job.get(\"brandName\"),\n",
    "                    \"è–ªèµ„\": job.get(\"salaryDesc\"),\n",
    "                    \"åœ°åŒº\": job.get(\"cityName\"),\n",
    "                    \"ç»éªŒ\": job.get(\"jobExperience\"),\n",
    "                    \"å­¦å†\": job.get(\"jobDegree\"),\n",
    "                    \"å…¬å¸è§„æ¨¡\": job.get(\"brandScaleName\"),\n",
    "                    \"è¡Œä¸š\": job.get(\"brandIndustry\"),\n",
    "                    \"ç¦åˆ©æ ‡ç­¾\": \",\".join(job.get(\"welfareList\", []) or []),\n",
    "                    \"æŠ€èƒ½æ ‡ç­¾\": \",\".join(job.get(\"skills\", []) or []),\n",
    "                    \"èŒä½æè¿°\": \"\",\n",
    "                    \"job_id\": job_id\n",
    "                })\n",
    "                self.seen_ids.add(job_id)\n",
    "            \n",
    "            print(f\"âœ… æœ¬é¡µé‡‘èç›¸å…³: {finance_count} | ç´¯è®¡: {len(self.data_list)}\")\n",
    "            \n",
    "            self.save_crawl_state(query, page + 1)\n",
    "            self.save_progress()\n",
    "            \n",
    "            # ç®€åŒ–ç­‰å¾…ï¼šå›ºå®š 2-4 ç§’\n",
    "            self._simple_sleep(2.0, 4.0)\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def get_job_detail(self, job_id):\n",
    "        \"\"\"è·å–èŒä½è¯¦æƒ…æè¿°\"\"\"\n",
    "        url = f\"https://www.zhipin.com/job_detail/{job_id}.html\"\n",
    "        \n",
    "        try:\n",
    "            resp = requests.get(url, headers=self.headers, timeout=15)\n",
    "            \n",
    "            if resp.status_code == 404:\n",
    "                return \"èŒä½å·²å…³é—­æˆ–ä¸å­˜åœ¨\"\n",
    "            if resp.status_code == 403:\n",
    "                print(f\"  âš ï¸ 403 ç¦æ­¢è®¿é—®\")\n",
    "                return None\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"  âš ï¸ çŠ¶æ€ç : {resp.status_code}\")\n",
    "                return None\n",
    "            \n",
    "            if 'security-check' in resp.url or 'verify' in resp.url:\n",
    "                print(f\"  âš ï¸ è§¦å‘å®‰å…¨éªŒè¯è·³è½¬\")\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            \n",
    "            selectors = [\n",
    "                '.job-sec-text', '.job-detail-section .text', '.job-sec .text',\n",
    "                '.job-description', '.job-detail .text', '[class*=\"job-sec\"] .text'\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                desc_tag = soup.select_one(selector)\n",
    "                if desc_tag and desc_tag.text.strip():\n",
    "                    return desc_tag.text.strip()\n",
    "            \n",
    "            if \"èŒä½æè¿°\" in resp.text:\n",
    "                print(f\"  âš ï¸ é¡µé¢å«å…³é”®è¯ä½†æœªåŒ¹é…åˆ°é€‰æ‹©å™¨\")\n",
    "            return \"\"\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  âš ï¸ è¯¦æƒ…é¡µè¯·æ±‚å¼‚å¸¸: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_progress(self):\n",
    "        \"\"\"ä»è¿›åº¦æ–‡ä»¶ä¸­åŠ è½½æ•°æ®\"\"\"\n",
    "        print(f\"\\nğŸ”„ æ­£åœ¨ä» {self.progress_file} åŠ è½½è¿›åº¦...\")\n",
    "        try:\n",
    "            df = pd.read_excel(self.progress_file)\n",
    "            df['èŒä½æè¿°'] = df['èŒä½æè¿°'].fillna('')\n",
    "            df['job_id'] = df['job_id'].fillna('')\n",
    "            \n",
    "            self.data_list = df.to_dict('records')\n",
    "            self.seen_ids = {item['job_id'] for item in self.data_list if item.get('job_id')}\n",
    "            \n",
    "            print(f\"âœ… åŠ è½½æˆåŠŸï¼Œå·²æ¢å¤ {len(self.data_list)} æ¡è®°å½•ã€‚\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"â„¹ï¸ æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "\n",
    "    def save_progress(self):\n",
    "        \"\"\"å°†å½“å‰æ•°æ®ä¿å­˜åˆ°è¿›åº¦æ–‡ä»¶\"\"\"\n",
    "        if not self.data_list:\n",
    "            return\n",
    "        try:\n",
    "            df = pd.DataFrame(self.data_list)\n",
    "            df.to_excel(self.progress_file, index=False, na_rep='')\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {e}\")\n",
    "\n",
    "    def fetch_all_details_resumable(self, start_index=0):\n",
    "        \"\"\"å¯æ¢å¤åœ°æŠ“å–æ‰€æœ‰è¯¦æƒ…\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸš€ (é˜¶æ®µ2) å¼€å§‹æŠ“å–èŒä½è¯¦ç»†æè¿°...\")\n",
    "        if start_index > 0:\n",
    "            print(f\"ğŸ“ è·³è¿‡å‰ {start_index} æ¡ï¼Œç›´æ¥ä»ç¬¬ {start_index + 1} æ¡å¼€å§‹\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        fetched_count = 0\n",
    "        consecutive_fail = 0\n",
    "        total_items = len(self.data_list)\n",
    "        \n",
    "        for i in range(start_index, total_items):\n",
    "            item = self.data_list[i]\n",
    "            \n",
    "            desc_str = str(item.get(\"èŒä½æè¿°\", \"\")).strip()\n",
    "            if desc_str and desc_str.lower() != 'nan' and len(desc_str) > 10:\n",
    "                continue\n",
    "            \n",
    "            print(f\"  ({i+1}/{total_items}) [æŠ“å–] {item['èŒä½']} - {item['å…¬å¸']}\")\n",
    "            \n",
    "            job_id = item.get(\"job_id\")\n",
    "            if not job_id:\n",
    "                continue\n",
    "            \n",
    "            new_desc = self.get_job_detail(job_id)\n",
    "            \n",
    "            if new_desc is None:\n",
    "                consecutive_fail += 1\n",
    "                print(f\"  âŒ æŠ“å–å¤±è´¥ (è¿ç»­: {consecutive_fail})\")\n",
    "                self.save_detail_state(i)\n",
    "                \n",
    "                if consecutive_fail >= 3:\n",
    "                    print(f\"\\n  ğŸ›‘ è¿ç»­å¤±è´¥è§¦å‘é£æ§ä¿æŠ¤ï¼Œåœæ­¢è¿è¡Œã€‚\")\n",
    "                    self.save_progress()\n",
    "                    return i\n",
    "                \n",
    "                self._simple_sleep(5.0, 10.0)\n",
    "                \n",
    "            elif new_desc == \"\":\n",
    "                consecutive_fail = 0\n",
    "                item[\"èŒä½æè¿°\"] = \"[æœªè·å–åˆ°æè¿°]\"\n",
    "                print(f\"  âš ï¸ æœªæ‰¾åˆ°æè¿°å†…å®¹\")\n",
    "                self._simple_sleep(3.0, 5.0)\n",
    "                \n",
    "            else:\n",
    "                consecutive_fail = 0\n",
    "                item[\"èŒä½æè¿°\"] = new_desc\n",
    "                fetched_count += 1\n",
    "                print(f\"  âœ… è·å–æˆåŠŸ\")\n",
    "                \n",
    "                if fetched_count % 5 == 0:\n",
    "                    self.save_progress()\n",
    "                    self.save_detail_state(i + 1)\n",
    "                \n",
    "                self._simple_sleep(4.0, 8.0)\n",
    "        \n",
    "        self.save_progress()\n",
    "        self.save_detail_state(total_items)\n",
    "        \n",
    "        print(\"\\nâœ… (é˜¶æ®µ2) è¯¦æƒ…æŠ“å–å®Œæ¯•ã€‚\")\n",
    "        if os.path.exists(self.detail_state_file):\n",
    "            os.remove(self.detail_state_file)\n",
    "        \n",
    "        return -1\n",
    "\n",
    "    def save_final_excel(self):\n",
    "        \"\"\"ä¿å­˜ä¸ºæœ€ç»ˆçš„ Excel æ–‡ä»¶\"\"\"\n",
    "        if not self.data_list:\n",
    "            return\n",
    "        \n",
    "        final_data = [item.copy() for item in self.data_list]\n",
    "        for item in final_data:\n",
    "            item.pop('job_id', None)\n",
    "                \n",
    "        df = pd.DataFrame(final_data)\n",
    "        filename = f\"é‡‘èè¡Œä¸šå²—ä½_å®Œæ•´ç‰ˆ_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.xlsx\"\n",
    "        df.to_excel(filename, index=False)\n",
    "        print(f\"\\nğŸ‰ å…¨éƒ¨å®Œæˆï¼å·²ä¿å­˜ï¼š{filename} (å…± {len(df)} æ¡)\")\n",
    "\n",
    "    def run(self, keywords, max_pages_per_kw=10, skip_list=False):\n",
    "        \"\"\"ä¸»è¿è¡Œé€»è¾‘\"\"\"\n",
    "        if os.path.exists(self.progress_file):\n",
    "            self.load_progress()\n",
    "        \n",
    "        if not skip_list:\n",
    "            start_kw_idx, start_page = self.load_crawl_state(keywords)\n",
    "            \n",
    "            print(f\"\\nğŸš€ (é˜¶æ®µ1) èŒä½åˆ—è¡¨æŠ“å–\")\n",
    "            for kw_idx in range(start_kw_idx, len(keywords)):\n",
    "                keyword = keywords[kw_idx]\n",
    "                page = start_page if kw_idx == start_kw_idx else 1\n",
    "                \n",
    "                print(f\"\\n>>> æ­£åœ¨æŠ“å–: {keyword} ({kw_idx+1}/{len(keywords)})\")\n",
    "                success = self.fetch_data(query=keyword, start_page=page, max_pages=max_pages_per_kw)\n",
    "                \n",
    "                if not success:\n",
    "                    print(\"\\nâš ï¸ ç¨‹åºæš‚åœï¼šè¯·æ›´æ–° Cookie åé‡æ–°è¿è¡Œ\")\n",
    "                    return\n",
    "                \n",
    "                self._simple_sleep(3.0, 6.0)\n",
    "            \n",
    "            if os.path.exists(self.state_file):\n",
    "                os.remove(self.state_file)\n",
    "        \n",
    "        start_detail_index = self.load_detail_state()\n",
    "        if self.data_list:\n",
    "            result = self.fetch_all_details_resumable(start_index=start_detail_index)\n",
    "            if result >= 0:\n",
    "                print(\"\\nâš ï¸ è¯¦æƒ…æŠ“å–æš‚åœï¼šè¯·æ›´æ–° Cookie åé‡æ–°è¿è¡Œ\")\n",
    "                return\n",
    "        \n",
    "        self.save_final_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a10626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ æ­£åœ¨ä» job_data_IN_PROGRESS.xlsx åŠ è½½è¿›åº¦...\n",
      "âœ… åŠ è½½æˆåŠŸï¼Œå·²æ¢å¤ 1979 æ¡è®°å½•ã€‚\n",
      "ğŸ“ æ¢å¤åˆ—è¡¨çˆ¬å–è¿›åº¦ï¼šå…³é”®è¯ [é“¶è¡Œ] ç¬¬ 10 é¡µ\n",
      "\n",
      "ğŸš€ (é˜¶æ®µ1) èŒä½åˆ—è¡¨æŠ“å–\n",
      "\n",
      ">>> æ­£åœ¨æŠ“å–: é“¶è¡Œ (1/9)\n",
      "\n",
      "ğŸ” [é“¶è¡Œ] ç¬¬ 10 é¡µ...\n",
      "âš ï¸ é£æ§è§¦å‘ï¼ä¿å­˜è¿›åº¦åé€€å‡º\n",
      "\n",
      "âš ï¸ ç¨‹åºæš‚åœï¼šè¯·æ›´æ–° Cookie åé‡æ–°è¿è¡Œ\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: è¿è¡Œçˆ¬è™«\n",
    "if __name__ == \"__main__\":\n",
    "    spider = BossSpiderAPI()\n",
    "    \n",
    "    # ä¼˜åŒ–ï¼šå»é™¤äº†é‡å¤çš„å…³é”®è¯\n",
    "    raw_keywords = [\"é“¶è¡Œ\", \"ä¿é™©\", \"æ•°æ®åˆ†æ\", \"åŸºé‡‘ç»ç†\", \"äº§å“\", \"è¿è¥\", \"è¯åˆ¸ç ”ç©¶æ‰€\", \"æŠ•è¡Œ\", \"é‡‘è\"]\n",
    "\n",
    "    # æ­£å¸¸è¿è¡Œ\n",
    "    spider.run(keywords=raw_keywords, max_pages_per_kw=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
