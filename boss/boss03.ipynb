{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d99273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d75f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ æ­£åœ¨ä» job_data_IN_PROGRESS.xlsx åŠ è½½è¿›åº¦...\n",
      "âœ… åŠ è½½æˆåŠŸï¼Œå·²æ¢å¤ 1755 æ¡è®°å½•ã€‚\n",
      "   å…¶ä¸­ 0 æ¡å·²æœ‰æè¿°ï¼Œ1755 æ¡å¾…æŠ“å–ã€‚\n",
      "ğŸ“ æ¢å¤çˆ¬å–è¿›åº¦ï¼šå…³é”®è¯ [é“¶è¡Œ] ç¬¬ 1 é¡µ\n",
      "\n",
      "ğŸš€ (é˜¶æ®µ1) å¼€å§‹/ç»§ç»­æŠ“å–èŒä½åˆ—è¡¨...\n",
      "\n",
      "========================================\n",
      "å…³é”®è¯ (1/9): é“¶è¡Œ\n",
      "========================================\n",
      "\n",
      "ğŸ” æŠ“å–å…³é”®è¯ [é“¶è¡Œ] åˆ—è¡¨ç¬¬ 1 é¡µ...\n",
      "âš ï¸ è§¦å‘é£æ§ (code: 37)ï¼\n",
      "ğŸ“ å½“å‰è¿›åº¦ï¼šå…³é”®è¯ [é“¶è¡Œ] ç¬¬ 1 é¡µ\n",
      "\n",
      "âš ï¸ è§¦å‘é£æ§ï¼Œå·²ä¿å­˜å½“å‰è¿›åº¦ã€‚\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "\n",
    "class BossSpiderAPI:\n",
    "    def __init__(self):\n",
    "        \"\"\"é‡‘èè¡Œä¸šèŒä½çˆ¬è™« - æ”¯æŒæ–­ç‚¹ç»­çˆ¬ (ä¼˜åŒ–ç‰ˆ)\"\"\"\n",
    "        self.base_url = \"https://www.zhipin.com/wapi/zpgeek/search/joblist.json\"\n",
    "        \n",
    "        # ä¼˜åŒ– 1: ä½¿ç”¨ Session å¯¹è±¡ï¼Œç»´æŒè¿æ¥æ± \n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # ä¼˜åŒ– 2: å‡†å¤‡ User-Agent æ± \n",
    "        self.ua_list = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0\"\n",
    "        ]\n",
    "\n",
    "        self.headers = {\n",
    "            \"User-Agent\": random.choice(self.ua_list),\n",
    "            \"Referer\": \"https://www.zhipin.com/web/geek/job\",\n",
    "            \"Accept\": \"application/json, text/plain, */*\",\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "            \"Origin\": \"https://www.zhipin.com\", # è¡¥å…… Origin\n",
    "            \"Accept-Language\": \"zh-CN,zh;q=0.9,en;q=0.8\", # è¡¥å……è¯­è¨€å¤´\n",
    "            # æ³¨æ„ï¼šCookie éœ€è¦å®šæœŸæ›´æ–°\n",
    "            \"Cookie\": \"ab_guid=78d319c7-13e6-4a47-baf0-f120a7373d6f; lastCity=100010000; wt2=Dkk1F98VQmqMn0UgGwTzC7o7OoHUbYqVCcZHsIQC0DajHRfhzMgYzgV38ljcbzyds0dOdNNbVyWhaO38G_gxErg~~; wbg=0; zp_at=LZazoHzQtE86zsLEFo3PmN_T2jRcXSEvG-dxQ9Lh-JI~; bst=V2R9MnF-L5219pVtRuyRgeLCm07DrVwig~|R9MnF-L5219pVtRuyRgeLCm07DrXwyQ~; __zp_stoken__=82aegw5ZfxIIcSRISbmsXEHhRw49VZFpTUMK%2BaGhaXFDCqcODw4bCjcKiwodVw4%2FDj8ONwpxVwrtbwrHCtsKkVsK6w4LCscODwq9SxINbwqrDiMKlxKvDvW3CsMKAWsOMwp9AOsONxIzFh8WGxYXFjMOtxIfFhsWFxYzDjcWHxYbFhcOgxI3Dn8OWxYXFjMWOxYfFhsWFxYxJTkBBS0dOP8K1xYxINkdETUzDjMKAw4Bmw4bCgcODb8OHwovDgmxJKiwWF2EaFhFjY2hqYm0eGGsdHWEdGhJsHhoTbWIdGmgSETQ%2Bw4vDhhNZGT4Sw4fDqBnDi0gTw5RiGy9Lw4LDkCYxQsSAVsKxQE%2FDhMWMTEBHQ01MTUAmR0tBTkNKREFANkvDh2vDl2saJUrDgkkvQ05HRE1MMmZDSkJDQENKQEFON0pEwo82TsOJwpfDhsOrQ0o%3D\"\n",
    "        }\n",
    "        \n",
    "        # æ›´æ–° Session çš„ headers\n",
    "        self.session.headers.update(self.headers)\n",
    "        \n",
    "        self.detail_headers = {\n",
    "            \"User-Agent\": self.headers[\"User-Agent\"],\n",
    "            \"Cookie\": self.headers[\"Cookie\"],\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\"\n",
    "        }\n",
    "        \n",
    "        self.finance_pattern = re.compile(\n",
    "            r\"(é‡‘è|é“¶è¡Œ|åˆ¸å•†|è¯åˆ¸|åŸºé‡‘|ä¿é™©|æœŸè´§|ä¿¡æ‰˜|æŠ•è¡Œ|èµ„ç®¡|ç†è´¢|è´¢å¯Œ|\"\n",
    "            r\"å°è´·|æ¶ˆé‡‘|ä¿ç†|ç§Ÿèµ|å¾ä¿¡|æ¸…ç®—|æ”¯ä»˜|äº’é‡‘|é‡‘èç§‘æŠ€|FinTech)\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        self.progress_file = \"job_data_IN_PROGRESS.xlsx\"\n",
    "        self.state_file = \"crawl_state.txt\"\n",
    "        \n",
    "        self.data_list = []\n",
    "        self.seen_ids = set()\n",
    "        \n",
    "        self.current_keyword_index = 0\n",
    "        self.current_page = 1\n",
    "\n",
    "    def is_finance_related(self, job):\n",
    "        \"\"\"åˆ¤æ–­æ˜¯å¦ä¸ºé‡‘èç›¸å…³èŒä½\"\"\"\n",
    "        text = \" \".join([\n",
    "            job.get(\"brandIndustry\", \"\") or \"\",\n",
    "            job.get(\"brandName\", \"\") or \"\",\n",
    "            job.get(\"jobName\", \"\") or \"\"\n",
    "        ])\n",
    "        return bool(self.finance_pattern.search(text))\n",
    "\n",
    "    def load_crawl_state(self, keywords):\n",
    "        \"\"\"åŠ è½½çˆ¬å–çŠ¶æ€ï¼ˆå½“å‰å…³é”®è¯å’Œé¡µç ï¼‰\"\"\"\n",
    "        if not os.path.exists(self.state_file):\n",
    "            return 0, 1\n",
    "        \n",
    "        try:\n",
    "            with open(self.state_file, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                keyword = lines[0].strip()\n",
    "                page = int(lines[1].strip())\n",
    "                \n",
    "                if keyword in keywords:\n",
    "                    idx = keywords.index(keyword)\n",
    "                    print(f\"ğŸ“ æ¢å¤çˆ¬å–è¿›åº¦ï¼šå…³é”®è¯ [{keyword}] ç¬¬ {page} é¡µ\")\n",
    "                    return idx, page\n",
    "                else:\n",
    "                    print(f\"âš ï¸ çŠ¶æ€æ–‡ä»¶ä¸­çš„å…³é”®è¯ [{keyword}] ä¸åœ¨å½“å‰åˆ—è¡¨ä¸­ï¼Œä»å¤´å¼€å§‹\")\n",
    "                    return 0, 1\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}ï¼Œä»å¤´å¼€å§‹\")\n",
    "            return 0, 1\n",
    "\n",
    "    def save_crawl_state(self, keyword, page):\n",
    "        \"\"\"ä¿å­˜çˆ¬å–çŠ¶æ€\"\"\"\n",
    "        try:\n",
    "            with open(self.state_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"{keyword}\\n{page}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ä¿å­˜çŠ¶æ€å¤±è´¥: {e}\")\n",
    "\n",
    "    def fetch_data(self, query=\"é‡‘è\", start_page=1, max_pages=5):\n",
    "        \"\"\"æŠ“å–èŒä½åˆ—è¡¨æ•°æ®\"\"\"\n",
    "        for page in range(start_page, max_pages + 1):\n",
    "            print(f\"\\nğŸ” æŠ“å–å…³é”®è¯ [{query}] åˆ—è¡¨ç¬¬ {page} é¡µ...\")\n",
    "            \n",
    "            params = {\n",
    "                \"scene\": \"1\",\n",
    "                \"query\": query,\n",
    "                \"city\": \"100010000\",\n",
    "                \"page\": page,\n",
    "                \"pageSize\": 30\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # ä¼˜åŒ– 4: ä½¿ç”¨ session å‘é€è¯·æ±‚\n",
    "                resp = self.session.post(\n",
    "                    self.base_url, \n",
    "                    data=params,\n",
    "                    timeout=15\n",
    "                )\n",
    "                resp.raise_for_status()\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"âŒ è¯·æ±‚å¤±è´¥ï¼š{e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = resp.json()\n",
    "                code = result.get(\"code\")\n",
    "                msg = result.get(\"message\") or result.get(\"msg\")\n",
    "                \n",
    "                if code == 37:\n",
    "                    print(f\"âš ï¸ è§¦å‘é£æ§ (code: 37)ï¼\")\n",
    "                    print(f\"ğŸ“ å½“å‰è¿›åº¦ï¼šå…³é”®è¯ [{query}] ç¬¬ {page} é¡µ\")\n",
    "                    self.save_crawl_state(query, page)\n",
    "                    self.save_progress()\n",
    "                    return False\n",
    "                elif code not in (0, \"0\", None):\n",
    "                    print(f\"âš ï¸ æ¥å£å¼‚å¸¸ code: {code}, message: {msg}\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ è§£æå“åº”å¤±è´¥: {e}\")\n",
    "                continue\n",
    "\n",
    "            job_list = result.get(\"zpData\", {}).get(\"jobList\", [])\n",
    "            \n",
    "            if not job_list:\n",
    "                print(\"âš ï¸ æœ¬é¡µæ— æ•°æ®ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªå…³é”®è¯\")\n",
    "                break\n",
    "            \n",
    "            print(f\"âœ… è·å–åˆ° {len(job_list)} ä¸ªèŒä½\")\n",
    "            \n",
    "            finance_count = 0\n",
    "            for job in job_list:\n",
    "                job_id = job.get(\"encryptJobId\")\n",
    "                \n",
    "                if not job_id or job_id in self.seen_ids:\n",
    "                    continue\n",
    "                \n",
    "                if not self.is_finance_related(job):\n",
    "                    continue\n",
    "                \n",
    "                finance_count += 1\n",
    "                \n",
    "                item = {\n",
    "                    \"èŒä½\": job.get(\"jobName\"),\n",
    "                    \"å…¬å¸\": job.get(\"brandName\"),\n",
    "                    \"è–ªèµ„\": job.get(\"salaryDesc\"),\n",
    "                    \"åœ°åŒº\": job.get(\"cityName\"),\n",
    "                    \"ç»éªŒ\": job.get(\"jobExperience\"),\n",
    "                    \"å­¦å†\": job.get(\"jobDegree\"),\n",
    "                    \"å…¬å¸è§„æ¨¡\": job.get(\"brandScaleName\"),\n",
    "                    \"è¡Œä¸š\": job.get(\"brandIndustry\"),\n",
    "                    \"ç¦åˆ©æ ‡ç­¾\": \",\".join(job.get(\"welfareList\", []) or []),\n",
    "                    \"æŠ€èƒ½æ ‡ç­¾\": \",\".join(job.get(\"skills\", []) or []),\n",
    "                    \"èŒä½æè¿°\": \"\",\n",
    "                    \"job_id\": job_id\n",
    "                }\n",
    "                \n",
    "                self.data_list.append(item)\n",
    "                self.seen_ids.add(job_id)\n",
    "            \n",
    "            print(f\"ğŸ“Š æœ¬é¡µç¬¦åˆæ¡ä»¶: {finance_count} ä¸ª | ç´¯è®¡: {len(self.data_list)} ä¸ª\")\n",
    "            \n",
    "            self.save_crawl_state(query, page + 1)\n",
    "            self.save_progress()\n",
    "            \n",
    "            time.sleep(random.uniform(2.0, 4.0))\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def get_job_detail(self, job_id):\n",
    "        \"\"\"è·å–èŒä½è¯¦æƒ…æè¿°\"\"\"\n",
    "        url = f\"https://www.zhipin.com/job_detail/{job_id}.html\"\n",
    "        try:\n",
    "            # ä¼˜åŒ– 5: ä½¿ç”¨ session è·å–è¯¦æƒ…\n",
    "            resp = self.session.get(url, headers=self.detail_headers, timeout=10)\n",
    "            if resp.status_code == 404:\n",
    "                return \"èŒä½å·²å…³é—­æˆ–ä¸å­˜åœ¨\"\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"  âš ï¸ è¯¦æƒ…é¡µçŠ¶æ€ç : {resp.status_code}\")\n",
    "                return None\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            desc_tag = soup.select_one('.job-sec-text')\n",
    "            return desc_tag.text.strip() if desc_tag else \"\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  âš ï¸ è¯¦æƒ…é¡µè¯·æ±‚å¤±è´¥: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_progress(self):\n",
    "        \"\"\"ä»è¿›åº¦æ–‡ä»¶ä¸­åŠ è½½æ•°æ®\"\"\"\n",
    "        print(f\"\\nğŸ”„ æ­£åœ¨ä» {self.progress_file} åŠ è½½è¿›åº¦...\")\n",
    "        try:\n",
    "            df = pd.read_excel(self.progress_file)\n",
    "            \n",
    "            df['èŒä½æè¿°'] = df['èŒä½æè¿°'].fillna('')\n",
    "            df['job_id'] = df['job_id'].fillna('')\n",
    "            \n",
    "            self.data_list = df.to_dict('records')\n",
    "            \n",
    "            for item in self.data_list:\n",
    "                if item.get('job_id'):\n",
    "                    self.seen_ids.add(item['job_id'])\n",
    "            \n",
    "            completed = sum(1 for item in self.data_list \n",
    "                           if item.get('èŒä½æè¿°') and \n",
    "                           str(item['èŒä½æè¿°']).strip() and \n",
    "                           str(item['èŒä½æè¿°']).strip().lower() != 'nan' and\n",
    "                           len(str(item['èŒä½æè¿°']).strip()) > 10)\n",
    "            \n",
    "            print(f\"âœ… åŠ è½½æˆåŠŸï¼Œå·²æ¢å¤ {len(self.data_list)} æ¡è®°å½•ã€‚\")\n",
    "            print(f\"   å…¶ä¸­ {completed} æ¡å·²æœ‰æè¿°ï¼Œ{len(self.data_list) - completed} æ¡å¾…æŠ“å–ã€‚\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"â„¹ï¸ æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "\n",
    "    def save_progress(self):\n",
    "        \"\"\"å°†å½“å‰æ•°æ®ä¿å­˜åˆ°è¿›åº¦æ–‡ä»¶\"\"\"\n",
    "        if not self.data_list:\n",
    "            return\n",
    "        try:\n",
    "            df = pd.DataFrame(self.data_list)\n",
    "            df.to_excel(self.progress_file, index=False, na_rep='')\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {e}\")\n",
    "\n",
    "    def fetch_all_details_resumable(self, start_index=0):\n",
    "        \"\"\"å¯æ¢å¤åœ°æŠ“å–æ‰€æœ‰è¯¦æƒ…\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸš€ (é˜¶æ®µ2) å¼€å§‹æŠ“å–èŒä½è¯¦ç»†æè¿°...\")\n",
    "        if start_index > 0:\n",
    "            print(f\"ğŸ“ ä»ç¬¬ {start_index + 1} æ¡å¼€å§‹ï¼ˆè·³è¿‡å‰ {start_index} æ¡ï¼‰\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        fetched_count = 0\n",
    "        skipped_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        # ä¼˜åŒ– 6: æ‰¹é‡ä¿å­˜è®¡æ•°å™¨\n",
    "        unsaved_changes = 0 \n",
    "        BATCH_SIZE = 10  # æ¯æŠ“å– 10 æ¡ä¿å­˜ä¸€æ¬¡\n",
    "        \n",
    "        for i, item in enumerate(self.data_list):\n",
    "            if i < start_index:\n",
    "                skipped_count += 1\n",
    "                if skipped_count % 100 == 0:\n",
    "                    print(f\"  [å¿«é€Ÿè·³è¿‡] å·²è·³è¿‡ {skipped_count} æ¡...\")\n",
    "                continue\n",
    "            \n",
    "            desc = item.get(\"èŒä½æè¿°\", \"\")\n",
    "            desc_str = str(desc).strip() if desc else \"\"\n",
    "            \n",
    "            if desc_str and desc_str.lower() != 'nan' and len(desc_str) > 10:\n",
    "                skipped_count += 1\n",
    "                if skipped_count % 20 == 1:\n",
    "                    print(f\"  ({i+1}/{len(self.data_list)}) [è·³è¿‡] å·²æœ‰æè¿°\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  ({i+1}/{len(self.data_list)}) [æŠ“å–] {item['èŒä½']} - {item['å…¬å¸']}\")\n",
    "            \n",
    "            job_id = item.get(\"job_id\")\n",
    "            if not job_id:\n",
    "                continue\n",
    "            \n",
    "            new_desc = self.get_job_detail(job_id)\n",
    "            \n",
    "            if new_desc is None:\n",
    "                failed_count += 1\n",
    "                print(\"  âŒ è¯¦æƒ…é¡µæŠ“å–å¤±è´¥ï¼Œå¯èƒ½å·²è§¦å‘é£æ§ï¼\")\n",
    "                \n",
    "                if failed_count >= 3:\n",
    "                    print(f\"\\n  ğŸ›‘ è¿ç»­å¤±è´¥ {failed_count} æ¬¡ï¼Œåœæ­¢æŠ“å–ã€‚\")\n",
    "                    self.save_progress()\n",
    "                    return i\n",
    "                else:\n",
    "                    print(f\"  âš ï¸ å°†ç»§ç»­å°è¯•ä¸‹ä¸€æ¡ï¼ˆå¤±è´¥è®¡æ•°ï¼š{failed_count}/3ï¼‰\")\n",
    "                    time.sleep(random.uniform(5.0, 8.0))\n",
    "                    continue\n",
    "            \n",
    "            failed_count = 0\n",
    "            item[\"èŒä½æè¿°\"] = new_desc\n",
    "            fetched_count += 1\n",
    "            unsaved_changes += 1\n",
    "            \n",
    "            # ä¼˜åŒ– 7: æ‰¹é‡ä¿å­˜é€»è¾‘\n",
    "            if unsaved_changes >= BATCH_SIZE:\n",
    "                print(f\"  ğŸ’¾ æ‰¹é‡ä¿å­˜è¿›åº¦ (å·²æ–°å¢ {unsaved_changes} æ¡)...\")\n",
    "                self.save_progress()\n",
    "                unsaved_changes = 0\n",
    "            \n",
    "            time.sleep(random.uniform(3.0, 6.0))\n",
    "        \n",
    "        if unsaved_changes > 0:\n",
    "            self.save_progress()\n",
    "            \n",
    "        print(\"\\nâœ… (é˜¶æ®µ2) è¯¦æƒ…æŠ“å–å®Œæ¯•ã€‚\")\n",
    "        print(f\"   è·³è¿‡: {skipped_count} æ¡ | æ–°å¢: {fetched_count} æ¡ | å¤±è´¥: {failed_count} æ¡\")\n",
    "        return -1\n",
    "\n",
    "    def save_final_excel(self):\n",
    "        \"\"\"ä¿å­˜ä¸ºæœ€ç»ˆçš„ Excel æ–‡ä»¶\"\"\"\n",
    "        if not self.data_list:\n",
    "            print(\"\\nâŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜\")\n",
    "            return\n",
    "        \n",
    "        final_data = [item.copy() for item in self.data_list]\n",
    "        for item in final_data:\n",
    "            if 'job_id' in item:\n",
    "                del item['job_id']\n",
    "                \n",
    "        df = pd.DataFrame(final_data)\n",
    "        \n",
    "        filename = f\"é‡‘èè¡Œä¸šå²—ä½_å®Œæ•´ç‰ˆ_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "        df.to_excel(filename, index=False)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ‰ å…¨éƒ¨å®Œæˆï¼å·²ä¿å­˜æœ€ç»ˆæ–‡ä»¶ï¼š{filename}\")\n",
    "        print(f\"    ğŸ“¦ å…± {len(df)} æ¡èŒä½\")\n",
    "        print(f\"    ğŸ“‹ åŒ…å«åˆ—: {', '.join(df.columns.tolist())}\")\n",
    "\n",
    "    def run(self, keywords=[\"é‡‘è\"], max_pages_per_kw=10, start_detail_index=0):\n",
    "        \"\"\"è¿è¡Œçˆ¬è™«\"\"\"\n",
    "        if os.path.exists(self.progress_file):\n",
    "            self.load_progress()\n",
    "        \n",
    "        start_kw_idx, start_page = self.load_crawl_state(keywords)\n",
    "        \n",
    "        print(f\"\\nğŸš€ (é˜¶æ®µ1) å¼€å§‹/ç»§ç»­æŠ“å–èŒä½åˆ—è¡¨...\")\n",
    "        \n",
    "        for kw_idx in range(start_kw_idx, len(keywords)):\n",
    "            keyword = keywords[kw_idx]\n",
    "            page = start_page if kw_idx == start_kw_idx else 1\n",
    "            \n",
    "            print(f\"\\n{'='*40}\")\n",
    "            print(f\"å…³é”®è¯ ({kw_idx+1}/{len(keywords)}): {keyword}\")\n",
    "            print(f\"{'='*40}\")\n",
    "            \n",
    "            success = self.fetch_data(query=keyword, start_page=page, max_pages=max_pages_per_kw)\n",
    "            \n",
    "            if not success:\n",
    "                print(\"\\nâš ï¸ è§¦å‘é£æ§ï¼Œå·²ä¿å­˜å½“å‰è¿›åº¦ã€‚\")\n",
    "                return\n",
    "            \n",
    "            if kw_idx < len(keywords) - 1:\n",
    "                time.sleep(random.uniform(3.0, 5.0))\n",
    "        \n",
    "        print(\"\\nğŸ‘ (é˜¶æ®µ1) èŒä½åˆ—è¡¨æŠ“å–å®Œæ¯•ã€‚\")\n",
    "        \n",
    "        if self.data_list:\n",
    "            self.fetch_all_details_resumable(start_index=start_detail_index)\n",
    "        \n",
    "        self.save_final_excel()\n",
    "        \n",
    "        if os.path.exists(self.state_file):\n",
    "            os.remove(self.state_file)\n",
    "            print(f\"   ğŸ—‘ï¸  å·²åˆ é™¤çŠ¶æ€æ–‡ä»¶ {self.state_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spider = BossSpiderAPI()\n",
    "    \n",
    "    keywords = [\"é“¶è¡Œ\",\"ä¿é™©\"]\n",
    "    \n",
    "    # æ¯ä¸ªå…³é”®è¯æŠ“å– 10# filepath: e:\\Programming\\ds and ai\\project\\Data-Science-and-AI\\boss\\boss.ipynb\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "\n",
    "class BossSpiderAPI:\n",
    "    def __init__(self):\n",
    "        \"\"\"é‡‘èè¡Œä¸šèŒä½çˆ¬è™« - æ”¯æŒæ–­ç‚¹ç»­çˆ¬ (ä¼˜åŒ–ç‰ˆ)\"\"\"\n",
    "        self.base_url = \"https://www.zhipin.com/wapi/zpgeek/search/joblist.json\"\n",
    "        \n",
    "        # ä¼˜åŒ– 1: ä½¿ç”¨ Session å¯¹è±¡ï¼Œç»´æŒè¿æ¥æ± \n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # ä¼˜åŒ– 2: å‡†å¤‡ User-Agent æ± \n",
    "        self.ua_list = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0\"\n",
    "        ]\n",
    "\n",
    "        self.headers = {\n",
    "            \"User-Agent\": random.choice(self.ua_list),\n",
    "            \"Referer\": \"https://www.zhipin.com/web/geek/job\",\n",
    "            \"Accept\": \"application/json, text/plain, */*\",\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "            \"Origin\": \"https://www.zhipin.com\", # è¡¥å…… Origin\n",
    "            \"Accept-Language\": \"zh-CN,zh;q=0.9,en;q=0.8\", # è¡¥å……è¯­è¨€å¤´\n",
    "            # æ³¨æ„ï¼šCookie éœ€è¦å®šæœŸæ›´æ–°\n",
    "            \"Cookie\": \"ab_guid=78d319c7-13e6-4a47-baf0-f120a7373d6f; lastCity=100010000; wt2=Dkk1F98VQmqMn0UgGwTzC7o7OoHUbYqVCcZHsIQC0DajHRfhzMgYzgV38ljcbzyds0dOdNNbVyWhaO38G_gxErg~~; wbg=0; zp_at=LZazoHzQtE86zsLEFo3PmN_T2jRcXSEvG-dxQ9Lh-JI~; bst=V2R9MnF-L5219pVtRuyRgYLSy27DrRzSk~|R9MnF-L5219pVtRuyRgYLSy27Drexyw~; __zp_stoken__=0a91gPEDDpMK5wpbCvUMvE0s8NEc8RzxAPT5BPEBfLUM5RThBQh9Xwr3Cig0kWMOIScK5Qzs9PEs8QEc8OkcrPUBDOUBDQ0PEusOFQEPCsFDEgUMsJcKEw4DCjw8iZMOVDMO6w4MNw6PCuA0pDWvCvQ1Gwrw7KBMPXRAXXmYUEBJgDxMVYxcWXBISZ19jYl1dChcQYwkTGSBLw67Cv8Kuwr3Ds8K9wqjDgcOywrzCrsOLOUU4OTM9xYbCrzFLQEdDPEs%2BxYbEv8S6xLzEv8WGxL%2FDisOUw7rDmsS%2FxLrEvMK6xYbEv8S6w7zDmsWGxL%2FEusS8w7nDhTY8wpvCucKiw6fCsV7DqsSnwp%2FCrsKTwqnCt1DCpcK6wp7CuMO8dMKzdsK3XMKtwqpMWsKZUGLCgMKYwofCusK4wp5KUFZhXcK0TE1XYU3DgVNxFRMRelgTQRBkPsOT\"\n",
    "        }\n",
    "        \n",
    "        # æ›´æ–° Session çš„ headers\n",
    "        self.session.headers.update(self.headers)\n",
    "        \n",
    "        self.detail_headers = {\n",
    "            \"User-Agent\": self.headers[\"User-Agent\"],\n",
    "            \"Cookie\": self.headers[\"Cookie\"],\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\"\n",
    "        }\n",
    "        \n",
    "        self.finance_pattern = re.compile(\n",
    "            r\"(é‡‘è|é“¶è¡Œ|åˆ¸å•†|è¯åˆ¸|åŸºé‡‘|ä¿é™©|æœŸè´§|ä¿¡æ‰˜|æŠ•è¡Œ|èµ„ç®¡|ç†è´¢|è´¢å¯Œ|\"\n",
    "            r\"å°è´·|æ¶ˆé‡‘|ä¿ç†|ç§Ÿèµ|å¾ä¿¡|æ¸…ç®—|æ”¯ä»˜|äº’é‡‘|é‡‘èç§‘æŠ€|FinTech)\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        self.progress_file = \"job_data_IN_PROGRESS.xlsx\"\n",
    "        self.state_file = \"crawl_state.txt\"\n",
    "        \n",
    "        self.data_list = []\n",
    "        self.seen_ids = set()\n",
    "        \n",
    "        self.current_keyword_index = 0\n",
    "        self.current_page = 1\n",
    "\n",
    "    def is_finance_related(self, job):\n",
    "        \"\"\"åˆ¤æ–­æ˜¯å¦ä¸ºé‡‘èç›¸å…³èŒä½\"\"\"\n",
    "        text = \" \".join([\n",
    "            job.get(\"brandIndustry\", \"\") or \"\",\n",
    "            job.get(\"brandName\", \"\") or \"\",\n",
    "            job.get(\"jobName\", \"\") or \"\"\n",
    "        ])\n",
    "        return bool(self.finance_pattern.search(text))\n",
    "\n",
    "    def load_crawl_state(self, keywords):\n",
    "        \"\"\"åŠ è½½çˆ¬å–çŠ¶æ€ï¼ˆå½“å‰å…³é”®è¯å’Œé¡µç ï¼‰\"\"\"\n",
    "        if not os.path.exists(self.state_file):\n",
    "            return 0, 1\n",
    "        \n",
    "        try:\n",
    "            with open(self.state_file, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                keyword = lines[0].strip()\n",
    "                page = int(lines[1].strip())\n",
    "                \n",
    "                if keyword in keywords:\n",
    "                    idx = keywords.index(keyword)\n",
    "                    print(f\"ğŸ“ æ¢å¤çˆ¬å–è¿›åº¦ï¼šå…³é”®è¯ [{keyword}] ç¬¬ {page} é¡µ\")\n",
    "                    return idx, page\n",
    "                else:\n",
    "                    print(f\"âš ï¸ çŠ¶æ€æ–‡ä»¶ä¸­çš„å…³é”®è¯ [{keyword}] ä¸åœ¨å½“å‰åˆ—è¡¨ä¸­ï¼Œä»å¤´å¼€å§‹\")\n",
    "                    return 0, 1\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}ï¼Œä»å¤´å¼€å§‹\")\n",
    "            return 0, 1\n",
    "\n",
    "    def save_crawl_state(self, keyword, page):\n",
    "        \"\"\"ä¿å­˜çˆ¬å–çŠ¶æ€\"\"\"\n",
    "        try:\n",
    "            with open(self.state_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"{keyword}\\n{page}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ä¿å­˜çŠ¶æ€å¤±è´¥: {e}\")\n",
    "\n",
    "    def fetch_data(self, query=\"é‡‘è\", start_page=1, max_pages=5):\n",
    "        \"\"\"æŠ“å–èŒä½åˆ—è¡¨æ•°æ®\"\"\"\n",
    "        for page in range(start_page, max_pages + 1):\n",
    "            print(f\"\\nğŸ” æŠ“å–å…³é”®è¯ [{query}] åˆ—è¡¨ç¬¬ {page} é¡µ...\")\n",
    "            \n",
    "            params = {\n",
    "                \"scene\": \"1\",\n",
    "                \"query\": query,\n",
    "                \"city\": \"100010000\",\n",
    "                \"page\": page,\n",
    "                \"pageSize\": 30\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # ä¼˜åŒ– 4: ä½¿ç”¨ session å‘é€è¯·æ±‚\n",
    "                resp = self.session.post(\n",
    "                    self.base_url, \n",
    "                    data=params,\n",
    "                    timeout=15\n",
    "                )\n",
    "                resp.raise_for_status()\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"âŒ è¯·æ±‚å¤±è´¥ï¼š{e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = resp.json()\n",
    "                code = result.get(\"code\")\n",
    "                msg = result.get(\"message\") or result.get(\"msg\")\n",
    "                \n",
    "                if code == 37:\n",
    "                    print(f\"âš ï¸ è§¦å‘é£æ§ (code: 37)ï¼\")\n",
    "                    print(f\"ğŸ“ å½“å‰è¿›åº¦ï¼šå…³é”®è¯ [{query}] ç¬¬ {page} é¡µ\")\n",
    "                    self.save_crawl_state(query, page)\n",
    "                    self.save_progress()\n",
    "                    return False\n",
    "                elif code not in (0, \"0\", None):\n",
    "                    print(f\"âš ï¸ æ¥å£å¼‚å¸¸ code: {code}, message: {msg}\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ è§£æå“åº”å¤±è´¥: {e}\")\n",
    "                continue\n",
    "\n",
    "            job_list = result.get(\"zpData\", {}).get(\"jobList\", [])\n",
    "            \n",
    "            if not job_list:\n",
    "                print(\"âš ï¸ æœ¬é¡µæ— æ•°æ®ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªå…³é”®è¯\")\n",
    "                break\n",
    "            \n",
    "            print(f\"âœ… è·å–åˆ° {len(job_list)} ä¸ªèŒä½\")\n",
    "            \n",
    "            finance_count = 0\n",
    "            for job in job_list:\n",
    "                job_id = job.get(\"encryptJobId\")\n",
    "                \n",
    "                if not job_id or job_id in self.seen_ids:\n",
    "                    continue\n",
    "                \n",
    "                if not self.is_finance_related(job):\n",
    "                    continue\n",
    "                \n",
    "                finance_count += 1\n",
    "                \n",
    "                item = {\n",
    "                    \"èŒä½\": job.get(\"jobName\"),\n",
    "                    \"å…¬å¸\": job.get(\"brandName\"),\n",
    "                    \"è–ªèµ„\": job.get(\"salaryDesc\"),\n",
    "                    \"åœ°åŒº\": job.get(\"cityName\"),\n",
    "                    \"ç»éªŒ\": job.get(\"jobExperience\"),\n",
    "                    \"å­¦å†\": job.get(\"jobDegree\"),\n",
    "                    \"å…¬å¸è§„æ¨¡\": job.get(\"brandScaleName\"),\n",
    "                    \"è¡Œä¸š\": job.get(\"brandIndustry\"),\n",
    "                    \"ç¦åˆ©æ ‡ç­¾\": \",\".join(job.get(\"welfareList\", []) or []),\n",
    "                    \"æŠ€èƒ½æ ‡ç­¾\": \",\".join(job.get(\"skills\", []) or []),\n",
    "                    \"èŒä½æè¿°\": \"\",\n",
    "                    \"job_id\": job_id\n",
    "                }\n",
    "                \n",
    "                self.data_list.append(item)\n",
    "                self.seen_ids.add(job_id)\n",
    "            \n",
    "            print(f\"ğŸ“Š æœ¬é¡µç¬¦åˆæ¡ä»¶: {finance_count} ä¸ª | ç´¯è®¡: {len(self.data_list)} ä¸ª\")\n",
    "            \n",
    "            self.save_crawl_state(query, page + 1)\n",
    "            self.save_progress()\n",
    "            \n",
    "            time.sleep(random.uniform(2.0, 4.0))\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def get_job_detail(self, job_id):\n",
    "        \"\"\"è·å–èŒä½è¯¦æƒ…æè¿°\"\"\"\n",
    "        url = f\"https://www.zhipin.com/job_detail/{job_id}.html\"\n",
    "        try:\n",
    "            # ä¼˜åŒ– 5: ä½¿ç”¨ session è·å–è¯¦æƒ…\n",
    "            resp = self.session.get(url, headers=self.detail_headers, timeout=10)\n",
    "            if resp.status_code == 404:\n",
    "                return \"èŒä½å·²å…³é—­æˆ–ä¸å­˜åœ¨\"\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"  âš ï¸ è¯¦æƒ…é¡µçŠ¶æ€ç : {resp.status_code}\")\n",
    "                return None\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            desc_tag = soup.select_one('.job-sec-text')\n",
    "            return desc_tag.text.strip() if desc_tag else \"\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  âš ï¸ è¯¦æƒ…é¡µè¯·æ±‚å¤±è´¥: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_progress(self):\n",
    "        \"\"\"ä»è¿›åº¦æ–‡ä»¶ä¸­åŠ è½½æ•°æ®\"\"\"\n",
    "        print(f\"\\nğŸ”„ æ­£åœ¨ä» {self.progress_file} åŠ è½½è¿›åº¦...\")\n",
    "        try:\n",
    "            df = pd.read_excel(self.progress_file)\n",
    "            \n",
    "            df['èŒä½æè¿°'] = df['èŒä½æè¿°'].fillna('')\n",
    "            df['job_id'] = df['job_id'].fillna('')\n",
    "            \n",
    "            self.data_list = df.to_dict('records')\n",
    "            \n",
    "            for item in self.data_list:\n",
    "                if item.get('job_id'):\n",
    "                    self.seen_ids.add(item['job_id'])\n",
    "            \n",
    "            completed = sum(1 for item in self.data_list \n",
    "                           if item.get('èŒä½æè¿°') and \n",
    "                           str(item['èŒä½æè¿°']).strip() and \n",
    "                           str(item['èŒä½æè¿°']).strip().lower() != 'nan' and\n",
    "                           len(str(item['èŒä½æè¿°']).strip()) > 10)\n",
    "            \n",
    "            print(f\"âœ… åŠ è½½æˆåŠŸï¼Œå·²æ¢å¤ {len(self.data_list)} æ¡è®°å½•ã€‚\")\n",
    "            print(f\"   å…¶ä¸­ {completed} æ¡å·²æœ‰æè¿°ï¼Œ{len(self.data_list) - completed} æ¡å¾…æŠ“å–ã€‚\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"â„¹ï¸ æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "\n",
    "    def save_progress(self):\n",
    "        \"\"\"å°†å½“å‰æ•°æ®ä¿å­˜åˆ°è¿›åº¦æ–‡ä»¶\"\"\"\n",
    "        if not self.data_list:\n",
    "            return\n",
    "        try:\n",
    "            df = pd.DataFrame(self.data_list)\n",
    "            df.to_excel(self.progress_file, index=False, na_rep='')\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {e}\")\n",
    "\n",
    "    def fetch_all_details_resumable(self, start_index=0):\n",
    "        \"\"\"å¯æ¢å¤åœ°æŠ“å–æ‰€æœ‰è¯¦æƒ…\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸš€ (é˜¶æ®µ2) å¼€å§‹æŠ“å–èŒä½è¯¦ç»†æè¿°...\")\n",
    "        if start_index > 0:\n",
    "            print(f\"ğŸ“ ä»ç¬¬ {start_index + 1} æ¡å¼€å§‹ï¼ˆè·³è¿‡å‰ {start_index} æ¡ï¼‰\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        fetched_count = 0\n",
    "        skipped_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        # ä¼˜åŒ– 6: æ‰¹é‡ä¿å­˜è®¡æ•°å™¨\n",
    "        unsaved_changes = 0 \n",
    "        BATCH_SIZE = 10  # æ¯æŠ“å– 10 æ¡ä¿å­˜ä¸€æ¬¡\n",
    "        \n",
    "        for i, item in enumerate(self.data_list):\n",
    "            if i < start_index:\n",
    "                skipped_count += 1\n",
    "                if skipped_count % 100 == 0:\n",
    "                    print(f\"  [å¿«é€Ÿè·³è¿‡] å·²è·³è¿‡ {skipped_count} æ¡...\")\n",
    "                continue\n",
    "            \n",
    "            desc = item.get(\"èŒä½æè¿°\", \"\")\n",
    "            desc_str = str(desc).strip() if desc else \"\"\n",
    "            \n",
    "            if desc_str and desc_str.lower() != 'nan' and len(desc_str) > 10:\n",
    "                skipped_count += 1\n",
    "                if skipped_count % 20 == 1:\n",
    "                    print(f\"  ({i+1}/{len(self.data_list)}) [è·³è¿‡] å·²æœ‰æè¿°\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  ({i+1}/{len(self.data_list)}) [æŠ“å–] {item['èŒä½']} - {item['å…¬å¸']}\")\n",
    "            \n",
    "            job_id = item.get(\"job_id\")\n",
    "            if not job_id:\n",
    "                continue\n",
    "            \n",
    "            new_desc = self.get_job_detail(job_id)\n",
    "            \n",
    "            if new_desc is None:\n",
    "                failed_count += 1\n",
    "                print(\"  âŒ è¯¦æƒ…é¡µæŠ“å–å¤±è´¥ï¼Œå¯èƒ½å·²è§¦å‘é£æ§ï¼\")\n",
    "                \n",
    "                if failed_count >= 3:\n",
    "                    print(f\"\\n  ğŸ›‘ è¿ç»­å¤±è´¥ {failed_count} æ¬¡ï¼Œåœæ­¢æŠ“å–ã€‚\")\n",
    "                    self.save_progress()\n",
    "                    return i\n",
    "                else:\n",
    "                    print(f\"  âš ï¸ å°†ç»§ç»­å°è¯•ä¸‹ä¸€æ¡ï¼ˆå¤±è´¥è®¡æ•°ï¼š{failed_count}/3ï¼‰\")\n",
    "                    time.sleep(random.uniform(5.0, 8.0))\n",
    "                    continue\n",
    "            \n",
    "            failed_count = 0\n",
    "            item[\"èŒä½æè¿°\"] = new_desc\n",
    "            fetched_count += 1\n",
    "            unsaved_changes += 1\n",
    "            \n",
    "            # ä¼˜åŒ– 7: æ‰¹é‡ä¿å­˜é€»è¾‘\n",
    "            if unsaved_changes >= BATCH_SIZE:\n",
    "                print(f\"  ğŸ’¾ æ‰¹é‡ä¿å­˜è¿›åº¦ (å·²æ–°å¢ {unsaved_changes} æ¡)...\")\n",
    "                self.save_progress()\n",
    "                unsaved_changes = 0\n",
    "            \n",
    "            time.sleep(random.uniform(3.0, 6.0))\n",
    "        \n",
    "        if unsaved_changes > 0:\n",
    "            self.save_progress()\n",
    "            \n",
    "        print(\"\\nâœ… (é˜¶æ®µ2) è¯¦æƒ…æŠ“å–å®Œæ¯•ã€‚\")\n",
    "        print(f\"   è·³è¿‡: {skipped_count} æ¡ | æ–°å¢: {fetched_count} æ¡ | å¤±è´¥: {failed_count} æ¡\")\n",
    "        return -1\n",
    "\n",
    "    def save_final_excel(self):\n",
    "        \"\"\"ä¿å­˜ä¸ºæœ€ç»ˆçš„ Excel æ–‡ä»¶\"\"\"\n",
    "        if not self.data_list:\n",
    "            print(\"\\nâŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜\")\n",
    "            return\n",
    "        \n",
    "        final_data = [item.copy() for item in self.data_list]\n",
    "        for item in final_data:\n",
    "            if 'job_id' in item:\n",
    "                del item['job_id']\n",
    "                \n",
    "        df = pd.DataFrame(final_data)\n",
    "        \n",
    "        filename = f\"é‡‘èè¡Œä¸šå²—ä½_å®Œæ•´ç‰ˆ_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "        df.to_excel(filename, index=False)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ‰ å…¨éƒ¨å®Œæˆï¼å·²ä¿å­˜æœ€ç»ˆæ–‡ä»¶ï¼š{filename}\")\n",
    "        print(f\"    ğŸ“¦ å…± {len(df)} æ¡èŒä½\")\n",
    "        print(f\"    ğŸ“‹ åŒ…å«åˆ—: {', '.join(df.columns.tolist())}\")\n",
    "\n",
    "    def run(self, keywords=[\"é‡‘è\"], max_pages_per_kw=10, start_detail_index=0):\n",
    "        \"\"\"è¿è¡Œçˆ¬è™«\"\"\"\n",
    "        if os.path.exists(self.progress_file):\n",
    "            self.load_progress()\n",
    "        \n",
    "        start_kw_idx, start_page = self.load_crawl_state(keywords)\n",
    "        \n",
    "        print(f\"\\nğŸš€ (é˜¶æ®µ1) å¼€å§‹/ç»§ç»­æŠ“å–èŒä½åˆ—è¡¨...\")\n",
    "        \n",
    "        for kw_idx in range(start_kw_idx, len(keywords)):\n",
    "            keyword = keywords[kw_idx]\n",
    "            page = start_page if kw_idx == start_kw_idx else 1\n",
    "            \n",
    "            print(f\"\\n{'='*40}\")\n",
    "            print(f\"å…³é”®è¯ ({kw_idx+1}/{len(keywords)}): {keyword}\")\n",
    "            print(f\"{'='*40}\")\n",
    "            \n",
    "            success = self.fetch_data(query=keyword, start_page=page, max_pages=max_pages_per_kw)\n",
    "            \n",
    "            if not success:\n",
    "                print(\"\\nâš ï¸ è§¦å‘é£æ§ï¼Œå·²ä¿å­˜å½“å‰è¿›åº¦ã€‚\")\n",
    "                return\n",
    "            \n",
    "            if kw_idx < len(keywords) - 1:\n",
    "                time.sleep(random.uniform(3.0, 5.0))\n",
    "        \n",
    "        print(\"\\nğŸ‘ (é˜¶æ®µ1) èŒä½åˆ—è¡¨æŠ“å–å®Œæ¯•ã€‚\")\n",
    "        \n",
    "        if self.data_list:\n",
    "            self.fetch_all_details_resumable(start_index=start_detail_index)\n",
    "        \n",
    "        self.save_final_excel()\n",
    "        \n",
    "        if os.path.exists(self.state_file):\n",
    "            os.remove(self.state_file)\n",
    "            print(f\"   ğŸ—‘ï¸  å·²åˆ é™¤çŠ¶æ€æ–‡ä»¶ {self.state_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spider = BossSpiderAPI()\n",
    "    \n",
    "    keywords = [\"é“¶è¡Œ\", \"ä¿é™©\", \"æ•°æ®åˆ†æ\", \"åŸºé‡‘ç»ç†\", \"äº§å“\", \"è¿è¥\", \"è¯åˆ¸ç ”ç©¶æ‰€\", \"æŠ•è¡Œ\", \"é‡‘è\"]\n",
    "    # ä¿æŒé¡ºåºå»é‡\n",
    "    \n",
    "    # æ¯ä¸ªå…³é”®è¯æŠ“å–\n",
    "    \n",
    "    spider.run(keywords=keywords, max_pages_per_kw=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
